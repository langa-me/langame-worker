{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "import datetime\n",
    "from langame import LangameClient\n",
    "c = LangameClient(path_to_config_file=\"../config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping id: 0ZcPZDcvuqgialj2wHpL, garbage\n",
      "Skipping id: 0m2ltwf7wkxLzcHnFkQE, garbage\n",
      "Skipping id: 1YWuwIp4srffwPNq7Aw9, garbage\n",
      "Skipping id: 4XlXSjPhH6QSGZDzwIQe, garbage\n",
      "Skipping id: 4sIjdMJr0exliT9yfvuL, garbage\n",
      "Skipping id: 5MAMBAYySyu3lKrR3zLt, garbage\n",
      "Skipping id: 6jNOw0e7VA6lHWddtqMy, garbage\n",
      "Skipping id: 7yHXwGPlHpopQAHU3xf1, garbage\n",
      "Skipping id: 8e9jVhvFpUIzj7ocNv06, garbage\n",
      "Skipping id: 8tzYKdv2Hkwxsm8eNQgj, garbage\n",
      "Skipping id: 9NdMRn8C285Eu3k4PdsS, garbage\n",
      "Skipping id: 9ZfEEkrB3cjLooW6YZM2, garbage\n",
      "Skipping id: B8hRGbGGOweqEyxuuplE, garbage\n",
      "Skipping id: BH9ZtGe1g6q3KmK7oV2f, garbage\n",
      "Skipping id: BZjXMPPKlmcqXG23i0mI, garbage\n",
      "Skipping id: BwSIpHO7X8N1fhf5zTyr, garbage\n",
      "Skipping id: C0yrKrOeX5kQKwEgyrJS, garbage\n",
      "Skipping id: CYsOWtp1ed2XDeplB5O5, garbage\n",
      "Skipping id: D8A4QcAlqidVArVPDKGP, garbage\n",
      "Skipping id: DhuuWoHv7oGKSA5agAzZ, garbage\n",
      "Skipping id: DjuEuISOCOG6R2D7ZlSy, garbage\n",
      "Skipping id: DlXglThZUiPJcuyLLKHV, garbage\n",
      "Skipping id: EBZCFk0aZPVS1SinJwcz, garbage\n",
      "Skipping id: EPBBhoxNvsJ7XdTcj4Qh, garbage\n",
      "Skipping id: FABbgBXx86NQywDLxTQ4, garbage\n",
      "Skipping id: FDT7XFDReB6ZBPkoQwID, garbage\n",
      "Skipping id: FP0UBU9PBzttN2d7YFvZ, garbage\n",
      "Skipping id: G1qq4aXPRbJoge3eLXop, garbage\n",
      "Skipping id: HGuXtX2VtbD3WEKo0kkr, garbage\n",
      "Skipping id: HSGlRuSzUIZ9GvswWelj, garbage\n",
      "Skipping id: Hm6wWR7NyyviL4VTnoIR, garbage\n",
      "Skipping id: HoNSOr041Z9GAlFUwecC, garbage\n",
      "Skipping id: ISOBHbvYDxi04VEFPZWw, garbage\n",
      "Skipping id: ITVfshoCDV8ZOWv9stEz, garbage\n",
      "Skipping id: Iu1De6gLJ0z7eoi7Qb96, garbage\n",
      "Skipping id: J5J8m3KWO5ti1n22kA8S, garbage\n",
      "Skipping id: NQVPkiiV6GRE7Pa2AVWm, garbage\n",
      "Skipping id: NtXdp7ODn4sR3YR2eEW6, garbage\n",
      "Skipping id: O6ZaqitDaFNwoJgIyLr8, garbage\n",
      "Skipping id: OqE5MuCnWDwtAbiinNrn, garbage\n",
      "Skipping id: P05T4keIh8bvF1SKVSSw, garbage\n",
      "Skipping id: QZHmodquL7nvyicTsHI9, garbage\n",
      "Skipping id: RCI4BcA6Bf1kYBk7hUTe, garbage\n",
      "Skipping id: RiYVQs2wMc9D2Q6hfQTt, garbage\n",
      "Skipping id: RwSNxqba5ZhA7Bj5eRsY, garbage\n",
      "Skipping id: U0DUXOSpfqYsgurOyL3L, garbage\n",
      "Skipping id: Uikgs6eO85MRivL0peL6, garbage\n",
      "Skipping id: VU7FcEfu3ebv66OjPam6, garbage\n",
      "Skipping id: VxUBlAJjUKV2wUq7cptD, garbage\n",
      "Skipping id: WFbsW55P5yN47vIzQAiK, garbage\n",
      "Skipping id: WRixt9bnIfzQ6xvgkI2b, garbage\n",
      "Skipping id: WZ2uSJ9YOgp29qJfg57i, garbage\n",
      "Skipping id: XIPyRwJloK6ZdAdTHOeu, garbage\n",
      "Skipping id: XdaHW0tJ9wIGAos7WoG4, garbage\n",
      "Skipping id: Y2V1ZLMVggTj45u8RmzK, garbage\n",
      "Skipping id: aGeDab69j51SPfrmBWvw, garbage\n",
      "Skipping id: ak1I3JXb3x0v1XhrQuAW, garbage\n",
      "Skipping id: bCW1uPMxDEy6iO3oMRQq, garbage\n",
      "Skipping id: bdf0W2iG9B9rZXbe4xun, garbage\n",
      "Skipping id: cYgeFDyGrepaF6G8ap6i, garbage\n",
      "Skipping id: d9FxG5haHnN2ku7spAng, garbage\n",
      "Skipping id: eyzSmD1AdfKkwOYtfoof, garbage\n",
      "Skipping id: f1ojlkGYgnOI4MLRGC4T, garbage\n",
      "Skipping id: fnprJBuBWAE9HH2LqHV6, garbage\n",
      "Skipping id: fySI8xbjfC5hx7QdHpva, garbage\n",
      "Skipping id: gLDxkmHFL3OFufeje6E8, garbage\n",
      "Skipping id: gUXUeStEjNZwDE1j5l8l, garbage\n",
      "Skipping id: gYsv5IG0NS1vkgdmgL2O, garbage\n",
      "Skipping id: hDybtBa855kDWy9l655v, garbage\n",
      "Skipping id: hoguvPcBPEnK4QbuXIp1, garbage\n",
      "Skipping id: i58xevUa8NEItnlX0Jzk, garbage\n",
      "Skipping id: idy5E0oY8I1HevGY7Dlk, garbage\n",
      "Skipping id: jkkOtT3PVoKQMmpA947Y, garbage\n",
      "Skipping id: kBa06RaHA2B9Drl3GtOB, garbage\n",
      "Skipping id: kI7uqiRAJkdBP7QTaTZX, garbage\n",
      "Skipping id: l1HjGNlNwpFzTnCsJxJA, garbage\n",
      "Skipping id: lzjy6lscTmTj1gcdIkNl, garbage\n",
      "Skipping id: o7jTZJyRoBxaDxIQwyQg, garbage\n",
      "Skipping id: osfrRcogOlIrEtxp5fpI, garbage\n",
      "Skipping id: pXXGv1o80FIldPiytsN7, garbage\n",
      "Skipping id: pf1tiPMceCwvXLUfKQwY, garbage\n",
      "Skipping id: pp3QqFslfmbgsdl3dOL8, garbage\n",
      "Skipping id: q2CEZCITA0RVSsfTQRP0, garbage\n",
      "Skipping id: qAwPXAX5efpoQt0TLoUu, garbage\n",
      "Skipping id: qXEJ0wGr86cw8CXIgL4z, garbage\n",
      "Skipping id: r5vxgYs9FnZYOxrh2tcj, garbage\n",
      "Skipping id: rdguN3u8mWRuiRXfUKM8, garbage\n",
      "Skipping id: sGpaBOsdY8MZGC87LzJf, garbage\n",
      "Skipping id: svY8fharA04Fga7FAki5, garbage\n",
      "Skipping id: svZxcinrbiybvsbqk3fT, garbage\n",
      "Skipping id: t58QkfO5GsncvNSmtpQi, garbage\n",
      "Skipping id: uKWTLomw35RqEQw8E6nb, garbage\n",
      "Skipping id: uTDyItbdhyWZoKV3Kc23, garbage\n",
      "Skipping id: uY5HtuacEZGzUXY4D4Lm, garbage\n",
      "Skipping id: ukpHKukvcNPlkhPaTjb5, garbage\n",
      "Skipping id: vDiOKT9fzERg9sasNjMA, garbage\n",
      "Skipping id: vPifbxb1aFBty4kTmrAF, garbage\n",
      "Skipping id: vp4TdnJzy1HbfgTBMIGU, garbage\n",
      "Skipping id: wHotBw9c4TWfO840fYIL, garbage\n",
      "Skipping id: wlKKB1Mkm7GZpnEQjp9w, garbage\n",
      "Skipping id: x2bjhNKDk1cIpaVFa9Ru, garbage\n",
      "Skipping id: xfXFHDkShv17nTMG2rk2, garbage\n",
      "Skipping id: yILwvdGuUBtf5h0U1FWM, garbage\n",
      "Skipping id: yYiXnfz1JA3zrikGBffE, garbage\n",
      "Skipping id: zm29pjekcPNeTWbaQn1U, garbage\n",
      "Skipping id: zmyvgX7sjisN5qDAN3y5, garbage\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.023083925247192383,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 391,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da2cac72f9cf4210b2b806cc4bf2c5ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/391 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.017854928970336914,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 190,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d5341927a5c4b8e9cadfdfabe39291c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010492801666259766,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 114,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2a1af43e9a94531a8fdeca0c5e400f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/114 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008408069610595703,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 2363431,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08cca75b02f84a20b42cab753e2ed7b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018976926803588867,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 1622,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6ba421618904c0cb2734679228d68cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018128156661987305,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 804,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c2d4b67ad54d3f8c1dd711fc989dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/804 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018549203872680664,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 122,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39a65ab0c7048a49af9154a07345331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00878000259399414,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 1883785969,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14e06ef5d154e138792f37346bdd1ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02149796485900879,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 112,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb660c895aad4c9dabcdd99de3186b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.025381088256835938,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 9621556,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c3b40403fe440ca2c0a16eb0fc0abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/9.62M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.023454904556274414,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 411,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1334deea267e46acb08704bc01f40420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02044200897216797,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 461,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b1b540b79024300beafd7d1629febcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/461 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/louisbeaumont/Documents/langame-worker/notebooks/fine_tune_generation-latest.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/louisbeaumont/Documents/langame-worker/notebooks/fine_tune_generation-latest.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlogging\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/louisbeaumont/Documents/langame-worker/notebooks/fine_tune_generation-latest.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m\"\u001b[39m\u001b[39mgeneration\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/louisbeaumont/Documents/langame-worker/notebooks/fine_tune_generation-latest.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m memes, index, embeddings_model \u001b[39m=\u001b[39m get_existing_conversation_starters(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/louisbeaumont/Documents/langame-worker/notebooks/fine_tune_generation-latest.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     c\u001b[39m.\u001b[39;49m_firestore_client, logger\u001b[39m=\u001b[39;49mlogger, confirmed\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/Documents/langame-worker/langame/conversation_starters.py:116\u001b[0m, in \u001b[0;36mget_existing_conversation_starters\u001b[0;34m(client, logger, use_gpu, limit, batch_embeddings_size, confirmed, push_to_hub)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mfor\u001b[39;00m folder \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39membeddings\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mindexes\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    115\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(folder):\n\u001b[0;32m--> 116\u001b[0m         shutil\u001b[39m.\u001b[39;49mrmtree(folder)\n\u001b[1;32m    118\u001b[0m \u001b[39mif\u001b[39;00m logger:\n\u001b[1;32m    119\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mSaving embeddings to disk and building index to disk\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/opt/python@3.8/Frameworks/Python.framework/Versions/3.8/lib/python3.8/shutil.py:709\u001b[0m, in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    707\u001b[0m     orig_st \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mlstat(path)\n\u001b[1;32m    708\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m--> 709\u001b[0m     onerror(os\u001b[39m.\u001b[39;49mlstat, path, sys\u001b[39m.\u001b[39;49mexc_info())\n\u001b[1;32m    710\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/opt/python@3.8/Frameworks/Python.framework/Versions/3.8/lib/python3.8/shutil.py:707\u001b[0m, in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39m# Note: To guard against symlink races, we use the standard\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[39m# lstat()/open()/fstat() trick.\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m     orig_st \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mlstat(path)\n\u001b[1;32m    708\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    709\u001b[0m     onerror(os\u001b[39m.\u001b[39mlstat, path, sys\u001b[39m.\u001b[39mexc_info())\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'embeddings'"
     ]
    }
   ],
   "source": [
    "from langame.conversation_starters import get_existing_conversation_starters\n",
    "import logging\n",
    "logger = logging.getLogger(\"generation\")\n",
    "memes, index, embeddings_model = get_existing_conversation_starters(\n",
    "    c._firestore_client, logger=logger, confirmed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file_name = f\"../data/fine_tune_generation_{datetime.date.today().strftime('%d_%m_%Y')}.jsonl\"\n",
    "\n",
    "for e in memes:\n",
    "    with open(out_file_name, \"a+\") as outfile:\n",
    "        json.dump({\n",
    "            \"prompt\": f\"{','.join(e['topics'])} ###\",\n",
    "            \"completion\": f\" {e['content']}\\n\",\n",
    "        }, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[34;1m\"prompt\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"personal,relationship,relationships,social,big talk,personal growth ###\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"completion\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\" Give eachother four praises and one critique\\n\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n",
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[34;1m\"prompt\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"ice breaker ###\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"completion\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\" When is a time when you know for sure you'll soon have to exchange a nice, meaningful conversation with someone new?\\n\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n",
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[34;1m\"prompt\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"space exploration,space travel ###\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"completion\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\" Do you think humans are the only intelligent life in the universe?\\n\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!head -n3 $out_file_name | jq ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "execute: openai tools fine_tunes.prepare_data -f /Users/louisbeaumont/Documents/langame-worker/notebooks/../data/fine_tune_generation_18_09_2022.jsonl\n"
     ]
    }
   ],
   "source": [
    "!echo \"execute: openai tools fine_tunes.prepare_data -f $(pwd)/$out_file_name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "train, test = train_test_split(\n",
    "    open(f\"{out_file_name.replace('.jsonl', '_prepared.jsonl')}\").read().splitlines(), test_size=0.1)\n",
    "with open(f\"{out_file_name.replace('.jsonl', '')}_prepared_train.jsonl\", \"w\") as outfile:\n",
    "    outfile.write(\"\\n\".join(train))\n",
    "with open(f\"{out_file_name.replace('.jsonl', '')}_prepared_test.jsonl\", \"w\") as outfile:\n",
    "    outfile.write(\"\\n\".join(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[34;1m\"prompt\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"transhumanism,public welfare ###\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"completion\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\" What do you think will happen if we continue to push the technological and social limits of humanity?\\n\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n",
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[34;1m\"prompt\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"ice breaker ###\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"completion\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\" What genre would your life story be if it was turned into a movie?\\n\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n",
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[34;1m\"prompt\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"ice breaker,emotions ###\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"completion\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\" If you could condense your own thoughts and feelings down to one word, what would that word be? Why?\\n\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!head -n3 {out_file_name.replace('.jsonl', '')}_prepared_train.jsonl | jq ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = openai.File.create(\n",
    "  file=open(f\"{out_file_name.replace('.jsonl', '')}_prepared_train.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "valid_file = openai.File.create(\n",
    "  file=open(f\"{out_file_name.replace('.jsonl', '')}_prepared_test.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "ft = openai.FineTune.create(\n",
    "    training_file=train_file[\"id\"],\n",
    "    validation_file=valid_file[\"id\"],\n",
    "    model=\"curie\",\n",
    "    suffix=\"generation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/louisbeaumont/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune ft-x3ojk3Kxs2Gkz9SJhE4m1fK6 has the status \"running\" and will not be logged\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ðŸŽ‰ wandb sync completed successfully'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai.wandb_logger import WandbLogger\n",
    "import wandb\n",
    "import re\n",
    "values = open(\"../.env.production\", \"r\").read()\n",
    "key = re.findall(r\"WANDB_KEY=\\\"(.*)\\\"\", values)[0]\n",
    "wandb.login(key=key, relogin=True)\n",
    "WandbLogger.sync(\n",
    "    id=ft[\"id\"],\n",
    "    project=\"generation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ft' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/louisbeaumont/Documents/langame-worker/notebooks/fine_tune_generation-latest.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/louisbeaumont/Documents/langame-worker/notebooks/fine_tune_generation-latest.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m results \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39mFineTune\u001b[39m.\u001b[39mretrieve(\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/louisbeaumont/Documents/langame-worker/notebooks/fine_tune_generation-latest.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     ft[\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/louisbeaumont/Documents/langame-worker/notebooks/fine_tune_generation-latest.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/louisbeaumont/Documents/langame-worker/notebooks/fine_tune_generation-latest.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m results\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ft' is not defined"
     ]
    }
   ],
   "source": [
    "results = openai.FineTune.retrieve(\n",
    "    ft[\"id\"],\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf _prepared*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b3c97174413c2100d2f2441a272ac4a9b6aae507e3bd1b85b4c1c7cd94685bf3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
