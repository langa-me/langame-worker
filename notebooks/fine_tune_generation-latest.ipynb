{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "import datetime\n",
    "from langame import LangameClient\n",
    "c = LangameClient(path_to_config_file=\"../config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/louisbeaumont/Documents/langame-worker/env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Skipping id: 0ZcPZDcvuqgialj2wHpL, garbage\n",
      "Skipping id: 4XlXSjPhH6QSGZDzwIQe, garbage\n",
      "Skipping id: 4sIjdMJr0exliT9yfvuL, garbage\n",
      "Skipping id: 5MAMBAYySyu3lKrR3zLt, garbage\n",
      "Skipping id: 6jNOw0e7VA6lHWddtqMy, garbage\n",
      "Skipping id: 7yHXwGPlHpopQAHU3xf1, garbage\n",
      "Skipping id: 8e9jVhvFpUIzj7ocNv06, garbage\n",
      "Skipping id: 8tzYKdv2Hkwxsm8eNQgj, garbage\n",
      "Skipping id: 9NdMRn8C285Eu3k4PdsS, garbage\n",
      "Skipping id: 9ZfEEkrB3cjLooW6YZM2, garbage\n",
      "Skipping id: B8hRGbGGOweqEyxuuplE, garbage\n",
      "Skipping id: BH9ZtGe1g6q3KmK7oV2f, garbage\n",
      "Skipping id: BZjXMPPKlmcqXG23i0mI, garbage\n",
      "Skipping id: CYsOWtp1ed2XDeplB5O5, garbage\n",
      "Skipping id: DlXglThZUiPJcuyLLKHV, garbage\n",
      "Skipping id: EBZCFk0aZPVS1SinJwcz, garbage\n",
      "Skipping id: EPBBhoxNvsJ7XdTcj4Qh, garbage\n",
      "Skipping id: FABbgBXx86NQywDLxTQ4, garbage\n",
      "Skipping id: FDT7XFDReB6ZBPkoQwID, garbage\n",
      "Skipping id: FP0UBU9PBzttN2d7YFvZ, garbage\n",
      "Skipping id: G1qq4aXPRbJoge3eLXop, garbage\n",
      "Skipping id: HGuXtX2VtbD3WEKo0kkr, garbage\n",
      "Skipping id: HSGlRuSzUIZ9GvswWelj, garbage\n",
      "Skipping id: HoNSOr041Z9GAlFUwecC, garbage\n",
      "Skipping id: ISOBHbvYDxi04VEFPZWw, garbage\n",
      "Skipping id: ITVfshoCDV8ZOWv9stEz, garbage\n",
      "Skipping id: Iu1De6gLJ0z7eoi7Qb96, garbage\n",
      "Skipping id: NtXdp7ODn4sR3YR2eEW6, garbage\n",
      "Skipping id: P05T4keIh8bvF1SKVSSw, garbage\n",
      "Skipping id: RCI4BcA6Bf1kYBk7hUTe, garbage\n",
      "Skipping id: RiYVQs2wMc9D2Q6hfQTt, garbage\n",
      "Skipping id: U0DUXOSpfqYsgurOyL3L, garbage\n",
      "Skipping id: Uikgs6eO85MRivL0peL6, garbage\n",
      "Skipping id: VU7FcEfu3ebv66OjPam6, garbage\n",
      "Skipping id: VxUBlAJjUKV2wUq7cptD, garbage\n",
      "Skipping id: WFbsW55P5yN47vIzQAiK, garbage\n",
      "Skipping id: WRixt9bnIfzQ6xvgkI2b, garbage\n",
      "Skipping id: WZ2uSJ9YOgp29qJfg57i, garbage\n",
      "Skipping id: XIPyRwJloK6ZdAdTHOeu, garbage\n",
      "Skipping id: XdaHW0tJ9wIGAos7WoG4, garbage\n",
      "Skipping id: Y2V1ZLMVggTj45u8RmzK, garbage\n",
      "Skipping id: aGeDab69j51SPfrmBWvw, garbage\n",
      "Skipping id: ak1I3JXb3x0v1XhrQuAW, garbage\n",
      "Skipping id: bCW1uPMxDEy6iO3oMRQq, garbage\n",
      "Skipping id: bdf0W2iG9B9rZXbe4xun, garbage\n",
      "Skipping id: cYgeFDyGrepaF6G8ap6i, garbage\n",
      "Skipping id: d9FxG5haHnN2ku7spAng, garbage\n",
      "Skipping id: eyzSmD1AdfKkwOYtfoof, garbage\n",
      "Skipping id: f1ojlkGYgnOI4MLRGC4T, garbage\n",
      "Skipping id: fnprJBuBWAE9HH2LqHV6, garbage\n",
      "Skipping id: fySI8xbjfC5hx7QdHpva, garbage\n",
      "Skipping id: gLDxkmHFL3OFufeje6E8, garbage\n",
      "Skipping id: gUXUeStEjNZwDE1j5l8l, garbage\n",
      "Skipping id: gYsv5IG0NS1vkgdmgL2O, garbage\n",
      "Skipping id: hDybtBa855kDWy9l655v, garbage\n",
      "Skipping id: hoguvPcBPEnK4QbuXIp1, garbage\n",
      "Skipping id: i58xevUa8NEItnlX0Jzk, garbage\n",
      "Skipping id: idy5E0oY8I1HevGY7Dlk, garbage\n",
      "Skipping id: jkkOtT3PVoKQMmpA947Y, garbage\n",
      "Skipping id: kBa06RaHA2B9Drl3GtOB, garbage\n",
      "Skipping id: kI7uqiRAJkdBP7QTaTZX, garbage\n",
      "Skipping id: l1HjGNlNwpFzTnCsJxJA, garbage\n",
      "Skipping id: lzjy6lscTmTj1gcdIkNl, garbage\n",
      "Skipping id: o7jTZJyRoBxaDxIQwyQg, garbage\n",
      "Skipping id: osfrRcogOlIrEtxp5fpI, garbage\n",
      "Skipping id: pXXGv1o80FIldPiytsN7, garbage\n",
      "Skipping id: pf1tiPMceCwvXLUfKQwY, garbage\n",
      "Skipping id: pp3QqFslfmbgsdl3dOL8, garbage\n",
      "Skipping id: q2CEZCITA0RVSsfTQRP0, garbage\n",
      "Skipping id: qAwPXAX5efpoQt0TLoUu, garbage\n",
      "Skipping id: qXEJ0wGr86cw8CXIgL4z, garbage\n",
      "Skipping id: r5vxgYs9FnZYOxrh2tcj, garbage\n",
      "Skipping id: rdguN3u8mWRuiRXfUKM8, garbage\n",
      "Skipping id: sGpaBOsdY8MZGC87LzJf, garbage\n",
      "Skipping id: svY8fharA04Fga7FAki5, garbage\n",
      "Skipping id: svZxcinrbiybvsbqk3fT, garbage\n",
      "Skipping id: t58QkfO5GsncvNSmtpQi, garbage\n",
      "Skipping id: uKWTLomw35RqEQw8E6nb, garbage\n",
      "Skipping id: uTDyItbdhyWZoKV3Kc23, garbage\n",
      "Skipping id: uY5HtuacEZGzUXY4D4Lm, garbage\n",
      "Skipping id: ukpHKukvcNPlkhPaTjb5, garbage\n",
      "Skipping id: vDiOKT9fzERg9sasNjMA, garbage\n",
      "Skipping id: vPifbxb1aFBty4kTmrAF, garbage\n",
      "Skipping id: vp4TdnJzy1HbfgTBMIGU, garbage\n",
      "Skipping id: wHotBw9c4TWfO840fYIL, garbage\n",
      "Skipping id: wlKKB1Mkm7GZpnEQjp9w, garbage\n",
      "Skipping id: x2bjhNKDk1cIpaVFa9Ru, garbage\n",
      "Skipping id: xfXFHDkShv17nTMG2rk2, garbage\n",
      "Skipping id: yILwvdGuUBtf5h0U1FWM, garbage\n",
      "Skipping id: yYiXnfz1JA3zrikGBffE, garbage\n",
      "Skipping id: zm29pjekcPNeTWbaQn1U, garbage\n",
      "Skipping id: zmyvgX7sjisN5qDAN3y5, garbage\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8 omp threads (processes), consider increasing --nb_cores if you have more\n",
      "Launching the whole pipeline 09/18/2022, 15:53:01\n",
      "There are 2922 embeddings of dim 768\n",
      "\tCompute estimated construction time of the index 09/18/2022, 15:53:01\n",
      "\t\t-> Train: 16.7 minutes\n",
      "\t\t-> Add: 0.0 seconds\n",
      "\t\tTotal: 16.7 minutes\n",
      "\t>>> Finished \"Compute estimated construction time of the index\" in 0.0001 secs\n",
      "\tChecking that your have enough memory available to create the index 09/18/2022, 15:53:01\n",
      "10.7MB of memory will be needed to build the index (more might be used if you have more)\n",
      "\t>>> Finished \"Checking that your have enough memory available to create the index\" in 0.0009 secs\n",
      "\tSelecting most promising index types given data characteristics 09/18/2022, 15:53:01\n",
      "\t>>> Finished \"Selecting most promising index types given data characteristics\" in 0.0000 secs\n",
      "\tCreating the index 09/18/2022, 15:53:01\n",
      "\t\t-> Instanciate the index HNSW15 09/18/2022, 15:53:01\n",
      "\t\t>>> Finished \"-> Instanciate the index HNSW15\" in 0.0035 secs\n",
      "The index size will be approximately 8.9MB\n",
      "The memory available for adding the vectors is 7.0GB(total available - used by the index)\n",
      "Will be using at most 1GB of ram for adding\n",
      "\t\t-> Adding the vectors to the index 09/18/2022, 15:53:01\n",
      "Using a batch size of 325520 (memory overhead 953.7MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 263.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t>>> Finished \"-> Adding the vectors to the index\" in 0.1068 secs\n",
      "\t>>> Finished \"Creating the index\" in 0.1121 secs\n",
      "\tComputing best hyperparameters 09/18/2022, 15:53:01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>>> Finished \"Computing best hyperparameters\" in 2.9018 secs\n",
      "The best hyperparameters are: efSearch=16383\n",
      "\tCompute fast metrics 09/18/2022, 15:53:04\n",
      "1456\n",
      "\t>>> Finished \"Compute fast metrics\" in 10.0316 secs\n",
      "\tSaving the index on local disk 09/18/2022, 15:53:14\n",
      "\t>>> Finished \"Saving the index on local disk\" in 0.0086 secs\n",
      "Recap:\n",
      "{'99p_search_speed_ms': 8.801004049998088,\n",
      " 'avg_search_speed_ms': 6.867052949862856,\n",
      " 'compression ratio': 0.9576711662625687,\n",
      " 'index_key': 'HNSW15',\n",
      " 'index_param': 'efSearch=16383',\n",
      " 'nb vectors': 2922,\n",
      " 'reconstruction error %': 0.0,\n",
      " 'size in bytes': 9373138,\n",
      " 'vectors dimension': 768}\n",
      ">>> Finished \"Launching the whole pipeline\" in 13.0729 secs\n"
     ]
    }
   ],
   "source": [
    "from langame.conversation_starters import get_existing_conversation_starters\n",
    "import logging\n",
    "logger = logging.getLogger(\"generation\")\n",
    "memes, index, embeddings_model = get_existing_conversation_starters(\n",
    "    c._firestore_client, logger=logger, confirmed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file_name = f\"../data/fine_tune_generation_{datetime.date.today().strftime('%d_%m_%Y')}.jsonl\"\n",
    "\n",
    "for e in memes:\n",
    "    with open(out_file_name, \"a+\") as outfile:\n",
    "        json.dump({\n",
    "            \"prompt\": f\"{','.join(e['topics'])} ###\",\n",
    "            \"completion\": f\" {e['content']}\\n\",\n",
    "        }, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[34;1m\"prompt\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"personal,relationship,relationships,social,big talk,personal growth ###\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"completion\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\" Give eachother four praises and one critique\\n\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n",
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[34;1m\"prompt\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"ice breaker ###\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"completion\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\" When is a time when you know for sure you'll soon have to exchange a nice, meaningful conversation with someone new?\\n\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n",
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[34;1m\"prompt\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"space exploration,space travel ###\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"completion\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\" Do you think humans are the only intelligent life in the universe?\\n\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!head -n3 $out_file_name | jq ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "execute: openai tools fine_tunes.prepare_data -f /Users/louisbeaumont/Documents/langame-worker/notebooks/../data/fine_tune_generation_18_09_2022.jsonl\n"
     ]
    }
   ],
   "source": [
    "!echo \"execute: openai tools fine_tunes.prepare_data -f $(pwd)/$out_file_name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "train, test = train_test_split(\n",
    "    open(f\"{out_file_name.replace('.jsonl', '_prepared.jsonl')}\").read().splitlines(), test_size=0.1)\n",
    "with open(f\"{out_file_name.replace('.jsonl', '')}_prepared_train.jsonl\", \"w\") as outfile:\n",
    "    outfile.write(\"\\n\".join(train))\n",
    "with open(f\"{out_file_name.replace('.jsonl', '')}_prepared_test.jsonl\", \"w\") as outfile:\n",
    "    outfile.write(\"\\n\".join(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[34;1m\"prompt\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"transhumanism,public welfare ###\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"completion\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\" What do you think will happen if we continue to push the technological and social limits of humanity?\\n\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n",
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[34;1m\"prompt\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"ice breaker ###\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"completion\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\" What genre would your life story be if it was turned into a movie?\\n\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n",
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[34;1m\"prompt\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"ice breaker,emotions ###\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"completion\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\" If you could condense your own thoughts and feelings down to one word, what would that word be? Why?\\n\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!head -n3 {out_file_name.replace('.jsonl', '')}_prepared_train.jsonl | jq ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = openai.File.create(\n",
    "  file=open(f\"{out_file_name.replace('.jsonl', '')}_prepared_train.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "valid_file = openai.File.create(\n",
    "  file=open(f\"{out_file_name.replace('.jsonl', '')}_prepared_test.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "ft = openai.FineTune.create(\n",
    "    training_file=train_file[\"id\"],\n",
    "    validation_file=valid_file[\"id\"],\n",
    "    model=\"curie\",\n",
    "    suffix=\"generation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/louisbeaumont/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune ft-x3ojk3Kxs2Gkz9SJhE4m1fK6 has the status \"running\" and will not be logged\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ðŸŽ‰ wandb sync completed successfully'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai.wandb_logger import WandbLogger\n",
    "import wandb\n",
    "import re\n",
    "values = open(\"../.env.production\", \"r\").read()\n",
    "key = re.findall(r\"WANDB_KEY=\\\"(.*)\\\"\", values)[0]\n",
    "wandb.login(key=key, relogin=True)\n",
    "WandbLogger.sync(\n",
    "    id=ft[\"id\"],\n",
    "    project=\"generation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ft' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/louisbeaumont/Documents/langame-worker/notebooks/fine_tune_generation-latest.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/louisbeaumont/Documents/langame-worker/notebooks/fine_tune_generation-latest.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m results \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39mFineTune\u001b[39m.\u001b[39mretrieve(\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/louisbeaumont/Documents/langame-worker/notebooks/fine_tune_generation-latest.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     ft[\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/louisbeaumont/Documents/langame-worker/notebooks/fine_tune_generation-latest.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/louisbeaumont/Documents/langame-worker/notebooks/fine_tune_generation-latest.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m results\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ft' is not defined"
     ]
    }
   ],
   "source": [
    "results = openai.FineTune.retrieve(\n",
    "    ft[\"id\"],\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf _prepared*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b3c97174413c2100d2f2441a272ac4a9b6aae507e3bd1b85b4c1c7cd94685bf3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
