{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0bjunyy4pPQH"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Skipping id: 0ZcPZDcvuqgialj2wHpL, garbage\n",
            "Skipping id: 0m2ltwf7wkxLzcHnFkQE, garbage\n",
            "Skipping id: 1YWuwIp4srffwPNq7Aw9, garbage\n",
            "Skipping id: 4XlXSjPhH6QSGZDzwIQe, garbage\n",
            "Skipping id: 4sIjdMJr0exliT9yfvuL, garbage\n",
            "Skipping id: 5MAMBAYySyu3lKrR3zLt, garbage\n",
            "Skipping id: 6jNOw0e7VA6lHWddtqMy, garbage\n",
            "Skipping id: 7yHXwGPlHpopQAHU3xf1, garbage\n",
            "Skipping id: 8e9jVhvFpUIzj7ocNv06, garbage\n",
            "Skipping id: 8tzYKdv2Hkwxsm8eNQgj, garbage\n",
            "Skipping id: 9NdMRn8C285Eu3k4PdsS, garbage\n",
            "Skipping id: 9ZfEEkrB3cjLooW6YZM2, garbage\n",
            "Skipping id: B8hRGbGGOweqEyxuuplE, garbage\n",
            "Skipping id: BH9ZtGe1g6q3KmK7oV2f, garbage\n",
            "Skipping id: BZjXMPPKlmcqXG23i0mI, garbage\n",
            "Skipping id: BwSIpHO7X8N1fhf5zTyr, garbage\n",
            "Skipping id: C0yrKrOeX5kQKwEgyrJS, garbage\n",
            "Skipping id: CYsOWtp1ed2XDeplB5O5, garbage\n",
            "Skipping id: D8A4QcAlqidVArVPDKGP, garbage\n",
            "Skipping id: DhuuWoHv7oGKSA5agAzZ, garbage\n",
            "Skipping id: DjuEuISOCOG6R2D7ZlSy, garbage\n",
            "Skipping id: DlXglThZUiPJcuyLLKHV, garbage\n",
            "Skipping id: EBZCFk0aZPVS1SinJwcz, garbage\n",
            "Skipping id: EPBBhoxNvsJ7XdTcj4Qh, garbage\n",
            "Skipping id: FABbgBXx86NQywDLxTQ4, garbage\n",
            "Skipping id: FDT7XFDReB6ZBPkoQwID, garbage\n",
            "Skipping id: FP0UBU9PBzttN2d7YFvZ, garbage\n",
            "Skipping id: G1qq4aXPRbJoge3eLXop, garbage\n",
            "Skipping id: HGuXtX2VtbD3WEKo0kkr, garbage\n",
            "Skipping id: HSGlRuSzUIZ9GvswWelj, garbage\n",
            "Skipping id: Hm6wWR7NyyviL4VTnoIR, garbage\n",
            "Skipping id: HoNSOr041Z9GAlFUwecC, garbage\n",
            "Skipping id: ISOBHbvYDxi04VEFPZWw, garbage\n",
            "Skipping id: ITVfshoCDV8ZOWv9stEz, garbage\n",
            "Skipping id: Iu1De6gLJ0z7eoi7Qb96, garbage\n",
            "Skipping id: J5J8m3KWO5ti1n22kA8S, garbage\n",
            "Skipping id: NQVPkiiV6GRE7Pa2AVWm, garbage\n",
            "Skipping id: NtXdp7ODn4sR3YR2eEW6, garbage\n",
            "Skipping id: O6ZaqitDaFNwoJgIyLr8, garbage\n",
            "Skipping id: OqE5MuCnWDwtAbiinNrn, garbage\n",
            "Skipping id: P05T4keIh8bvF1SKVSSw, garbage\n",
            "Skipping id: QZHmodquL7nvyicTsHI9, garbage\n",
            "Skipping id: RCI4BcA6Bf1kYBk7hUTe, garbage\n",
            "Skipping id: RiYVQs2wMc9D2Q6hfQTt, garbage\n",
            "Skipping id: RwSNxqba5ZhA7Bj5eRsY, garbage\n",
            "Skipping id: U0DUXOSpfqYsgurOyL3L, garbage\n",
            "Skipping id: Uikgs6eO85MRivL0peL6, garbage\n",
            "Skipping id: VU7FcEfu3ebv66OjPam6, garbage\n",
            "Skipping id: VxUBlAJjUKV2wUq7cptD, garbage\n",
            "Skipping id: WFbsW55P5yN47vIzQAiK, garbage\n",
            "Skipping id: WRixt9bnIfzQ6xvgkI2b, garbage\n",
            "Skipping id: WZ2uSJ9YOgp29qJfg57i, garbage\n",
            "Skipping id: XIPyRwJloK6ZdAdTHOeu, garbage\n",
            "Skipping id: XdaHW0tJ9wIGAos7WoG4, garbage\n",
            "Skipping id: Y2V1ZLMVggTj45u8RmzK, garbage\n",
            "Skipping id: aGeDab69j51SPfrmBWvw, garbage\n",
            "Skipping id: ak1I3JXb3x0v1XhrQuAW, garbage\n",
            "Skipping id: bCW1uPMxDEy6iO3oMRQq, garbage\n",
            "Skipping id: bdf0W2iG9B9rZXbe4xun, garbage\n",
            "Skipping id: cYgeFDyGrepaF6G8ap6i, garbage\n",
            "Skipping id: d9FxG5haHnN2ku7spAng, garbage\n",
            "Skipping id: eyzSmD1AdfKkwOYtfoof, garbage\n",
            "Skipping id: f1ojlkGYgnOI4MLRGC4T, garbage\n",
            "Skipping id: fnprJBuBWAE9HH2LqHV6, garbage\n",
            "Skipping id: fySI8xbjfC5hx7QdHpva, garbage\n",
            "Skipping id: gLDxkmHFL3OFufeje6E8, garbage\n",
            "Skipping id: gUXUeStEjNZwDE1j5l8l, garbage\n",
            "Skipping id: gYsv5IG0NS1vkgdmgL2O, garbage\n",
            "Skipping id: hDybtBa855kDWy9l655v, garbage\n",
            "Skipping id: hoguvPcBPEnK4QbuXIp1, garbage\n",
            "Skipping id: i58xevUa8NEItnlX0Jzk, garbage\n",
            "Skipping id: idy5E0oY8I1HevGY7Dlk, garbage\n",
            "Skipping id: jkkOtT3PVoKQMmpA947Y, garbage\n",
            "Skipping id: kBa06RaHA2B9Drl3GtOB, garbage\n",
            "Skipping id: kI7uqiRAJkdBP7QTaTZX, garbage\n",
            "Skipping id: l1HjGNlNwpFzTnCsJxJA, garbage\n",
            "Skipping id: lzjy6lscTmTj1gcdIkNl, garbage\n",
            "Skipping id: o7jTZJyRoBxaDxIQwyQg, garbage\n",
            "Skipping id: osfrRcogOlIrEtxp5fpI, garbage\n",
            "Skipping id: pXXGv1o80FIldPiytsN7, garbage\n",
            "Skipping id: pf1tiPMceCwvXLUfKQwY, garbage\n",
            "Skipping id: pp3QqFslfmbgsdl3dOL8, garbage\n",
            "Skipping id: q2CEZCITA0RVSsfTQRP0, garbage\n",
            "Skipping id: qAwPXAX5efpoQt0TLoUu, garbage\n",
            "Skipping id: qXEJ0wGr86cw8CXIgL4z, garbage\n",
            "Skipping id: r5vxgYs9FnZYOxrh2tcj, garbage\n",
            "Skipping id: rdguN3u8mWRuiRXfUKM8, garbage\n",
            "Skipping id: sGpaBOsdY8MZGC87LzJf, garbage\n",
            "Skipping id: svY8fharA04Fga7FAki5, garbage\n",
            "Skipping id: svZxcinrbiybvsbqk3fT, garbage\n",
            "Skipping id: t58QkfO5GsncvNSmtpQi, garbage\n",
            "Skipping id: uKWTLomw35RqEQw8E6nb, garbage\n",
            "Skipping id: uTDyItbdhyWZoKV3Kc23, garbage\n",
            "Skipping id: uY5HtuacEZGzUXY4D4Lm, garbage\n",
            "Skipping id: ukpHKukvcNPlkhPaTjb5, garbage\n",
            "Skipping id: vDiOKT9fzERg9sasNjMA, garbage\n",
            "Skipping id: vPifbxb1aFBty4kTmrAF, garbage\n",
            "Skipping id: vp4TdnJzy1HbfgTBMIGU, garbage\n",
            "Skipping id: wHotBw9c4TWfO840fYIL, garbage\n",
            "Skipping id: wlKKB1Mkm7GZpnEQjp9w, garbage\n",
            "Skipping id: x2bjhNKDk1cIpaVFa9Ru, garbage\n",
            "Skipping id: xfXFHDkShv17nTMG2rk2, garbage\n",
            "Skipping id: yILwvdGuUBtf5h0U1FWM, garbage\n",
            "Skipping id: yYiXnfz1JA3zrikGBffE, garbage\n",
            "Skipping id: zm29pjekcPNeTWbaQn1U, garbage\n",
            "Skipping id: zmyvgX7sjisN5qDAN3y5, garbage\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using 8 omp threads (processes), consider increasing --nb_cores if you have more\n",
            "Launching the whole pipeline 11/06/2022, 14:30:50\n",
            "There are 3059 embeddings of dim 768\n",
            "\tCompute estimated construction time of the index 11/06/2022, 14:30:50\n",
            "\t\t-> Train: 16.7 minutes\n",
            "\t\t-> Add: 0.0 seconds\n",
            "\t\tTotal: 16.7 minutes\n",
            "\t>>> Finished \"Compute estimated construction time of the index\" in 0.0002 secs\n",
            "\tChecking that your have enough memory available to create the index 11/06/2022, 14:30:50\n",
            "11.2MB of memory will be needed to build the index (more might be used if you have more)\n",
            "\t>>> Finished \"Checking that your have enough memory available to create the index\" in 0.0016 secs\n",
            "\tSelecting most promising index types given data characteristics 11/06/2022, 14:30:50\n",
            "\t>>> Finished \"Selecting most promising index types given data characteristics\" in 0.0000 secs\n",
            "\tCreating the index 11/06/2022, 14:30:50\n",
            "\t\t-> Instanciate the index HNSW15 11/06/2022, 14:30:50\n",
            "\t\t>>> Finished \"-> Instanciate the index HNSW15\" in 0.0044 secs\n",
            "The index size will be approximately 9.3MB\n",
            "The memory available for adding the vectors is 7.0GB(total available - used by the index)\n",
            "Will be using at most 1GB of ram for adding\n",
            "\t\t-> Adding the vectors to the index 11/06/2022, 14:30:50\n",
            "Using a batch size of 325520 (memory overhead 953.7MB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 235.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\t>>> Finished \"-> Adding the vectors to the index\" in 0.0941 secs\n",
            "\t>>> Finished \"Creating the index\" in 0.0996 secs\n",
            "\tComputing best hyperparameters 11/06/2022, 14:30:50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t>>> Finished \"Computing best hyperparameters\" in 4.0102 secs\n",
            "The best hyperparameters are: efSearch=8885\n",
            "\tCompute fast metrics 11/06/2022, 14:30:54\n",
            "1052\n",
            "\t>>> Finished \"Compute fast metrics\" in 10.0376 secs\n",
            "\tSaving the index on local disk 11/06/2022, 14:31:04\n",
            "\t>>> Finished \"Saving the index on local disk\" in 0.0251 secs\n",
            "Recap:\n",
            "{'99p_search_speed_ms': 43.29755142000498,\n",
            " 'avg_search_speed_ms': 9.505365207224562,\n",
            " 'compression ratio': 0.9576690276384732,\n",
            " 'index_key': 'HNSW15',\n",
            " 'index_param': 'efSearch=8885',\n",
            " 'nb vectors': 3059,\n",
            " 'reconstruction error %': 0.0,\n",
            " 'size in bytes': 9812626,\n",
            " 'vectors dimension': 768}\n",
            ">>> Finished \"Launching the whole pipeline\" in 14.1983 secs\n"
          ]
        }
      ],
      "source": [
        "from langame import LangameClient\n",
        "import json\n",
        "import openai\n",
        "import datetime\n",
        "from langame.conversation_starters import get_existing_conversation_starters\n",
        "import logging\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer, AutoTokenizer, DataCollatorForLanguageModeling, DataCollatorForLanguageModeling\n",
        "\n",
        "c = LangameClient(\"../config.yaml\")\n",
        "from langame.conversation_starters import get_existing_conversation_starters\n",
        "import logging\n",
        "logger = logging.getLogger(\"classification\")\n",
        "memes, index, embeddings_model = get_existing_conversation_starters(\n",
        "    c._firestore_client, logger=logger, confirmed=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "J9zcl6xrpTQa"
      },
      "outputs": [],
      "source": [
        "dataset = [(),()]\n",
        "\n",
        "for e in memes:\n",
        "  p = f\"{','.join(e['topics'])} ### {e['content']}\"\n",
        "  dataset[0] += (p,)\n",
        "  dataset[1] += (100.0,)\n",
        "for e in c._firestore_client.collection(\"deleted_memes\").stream():\n",
        "  # check has \"topics\" and \"content\"\n",
        "  if \"topics\" not in e.to_dict() or \"content\" not in e.to_dict():\n",
        "    continue\n",
        "  e = e.to_dict()\n",
        "  p = f\"{','.join(e['topics'])} ### {e['content']}\"\n",
        "  dataset[0] += (p,)\n",
        "  dataset[1] += (1.0,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "sCgIns9WOlQp",
        "outputId": "0d5c2862-50ce-4f37-b779-92d095d11e80"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "NOTE: Redirects are currently not supported in Windows or MacOs.\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "module 'trlx' has no attribute 'train'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[39m=\u001b[39m GPT2Tokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mdistilgpt2\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[39m# Steer a model with a collection of rated samples\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m model \u001b[39m=\u001b[39m trlx\u001b[39m.\u001b[39;49mtrain(\u001b[39m'\u001b[39m\u001b[39mdistilgpt2\u001b[39m\u001b[39m'\u001b[39m, dataset\u001b[39m=\u001b[39mdataset)\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'trlx' has no attribute 'train'"
          ]
        }
      ],
      "source": [
        "import trlx\n",
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
        "\n",
        "# Steer a model with a collection of rated samples\n",
        "model = trlx.train('distilgpt2', dataset=dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEd7FDHjSPZW"
      },
      "outputs": [],
      "source": [
        "# model is a wrapper with some logit preprocessing\n",
        "model.generate(**tokenizer('Q: Who rules the world? A:', return_tensors='pt'), do_sample=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.15 ('env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "b3c97174413c2100d2f2441a272ac4a9b6aae507e3bd1b85b4c1c7cd94685bf3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
