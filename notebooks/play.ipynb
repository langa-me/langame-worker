{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langame.langame_client import LangameClient\n",
    "from langame.array import intersection\n",
    "from pprint import pprint\n",
    "from firebase_admin import firestore\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import openai\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = LangameClient(path_to_config_file=\"../config.yaml\")\n",
    "memes = []\n",
    "for e in c._firestore_client.collection('memes').stream():\n",
    "    memes.append((e.id, e.to_dict()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000000000000 [00:04<1276401612:29:32,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"content\": \"What is one thing you would never think of telling someone else?\", \"topics\": [\"mind\", \"ice breaker\"]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1000000000000 [00:09<1289429473:43:06,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"content\": \"What is the joke that you've heard the most times and always find yourself laughing?\", \"topics\": [\"other\"]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1000000000000 [00:14<1335375518:33:19,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"content\": \"What two or three things should a company hire people in advance to guarantee they're going to be successful?\", \"topics\": [\"ice breaker\"]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1000000000000 [00:19<1396712947:39:21,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"content\": \"What \\u201cun-cool\\u201d thing are you most guilty of doing as a kid?\", \"topics\": [\"ice breaker\"]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1000000000000 [00:20<1437050534:36:48,  5.17s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_309798/2533763492.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ice breaker'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "from random import randint\n",
    "def get_prompt(topics):\n",
    "    rnd = random.choices(memes, k=500)\n",
    "    return [{\"content\": e[1]['content'], \"topics\": e[1]['topics']} for e in rnd if len(intersection(e[1]['topics'], topics)) > 0]\n",
    "outputs = []\n",
    "for i in tqdm(range(10**12)):\n",
    "    time.sleep(randint(1,3))\n",
    "    samples = get_prompt(['ice breaker'])\n",
    "    p = \"\\n\".join([json.dumps(e) for e in samples[0:60]])\n",
    "    try:\n",
    "        response = openai.Completion.create(\n",
    "            engine=\"davinci-codex\",\n",
    "            prompt=p+\"\\n\",\n",
    "            temperature=1,\n",
    "            max_tokens=100,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0.7,\n",
    "            presence_penalty=0,\n",
    "            stop=[\"\\n\"]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n",
    "        continue\n",
    "    text = response[\"choices\"][0][\"text\"]\n",
    "    outputs.append(text)\n",
    "    # every 10 samples, append the generated samples to file\n",
    "    if i % 10 == 0:\n",
    "        with open(\"generated_samples.txt\", \"a\") as f:\n",
    "            f.write(\"\\n\".join(outputs) + \"\\n\")\n",
    "            outputs = []"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Overview of Colaboratory Features",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "2ca8946d339ea3bbaa73adc6693d06c322e424e77befb9c6b521a96205a94f7d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('.venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
