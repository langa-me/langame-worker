{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langame import LangameClient\n",
    "import json\n",
    "import openai\n",
    "import datetime\n",
    "from langame.conversation_starters import get_existing_conversation_starters\n",
    "import logging\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer, AutoTokenizer, DataCollatorForLanguageModeling, DataCollatorForLanguageModeling\n",
    "\n",
    "c = LangameClient(\"../config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# quality classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping id: 0ZcPZDcvuqgialj2wHpL, garbage\n",
      "Skipping id: 0m2ltwf7wkxLzcHnFkQE, garbage\n",
      "Skipping id: 1YWuwIp4srffwPNq7Aw9, garbage\n",
      "Skipping id: 4XlXSjPhH6QSGZDzwIQe, garbage\n",
      "Skipping id: 4sIjdMJr0exliT9yfvuL, garbage\n",
      "Skipping id: 5MAMBAYySyu3lKrR3zLt, garbage\n",
      "Skipping id: 6jNOw0e7VA6lHWddtqMy, garbage\n",
      "Skipping id: 7yHXwGPlHpopQAHU3xf1, garbage\n",
      "Skipping id: 8e9jVhvFpUIzj7ocNv06, garbage\n",
      "Skipping id: 8tzYKdv2Hkwxsm8eNQgj, garbage\n",
      "Skipping id: 9NdMRn8C285Eu3k4PdsS, garbage\n",
      "Skipping id: 9ZfEEkrB3cjLooW6YZM2, garbage\n",
      "Skipping id: B8hRGbGGOweqEyxuuplE, garbage\n",
      "Skipping id: BH9ZtGe1g6q3KmK7oV2f, garbage\n",
      "Skipping id: BZjXMPPKlmcqXG23i0mI, garbage\n",
      "Skipping id: BwSIpHO7X8N1fhf5zTyr, garbage\n",
      "Skipping id: C0yrKrOeX5kQKwEgyrJS, garbage\n",
      "Skipping id: CYsOWtp1ed2XDeplB5O5, garbage\n",
      "Skipping id: D8A4QcAlqidVArVPDKGP, garbage\n",
      "Skipping id: DhuuWoHv7oGKSA5agAzZ, garbage\n",
      "Skipping id: DjuEuISOCOG6R2D7ZlSy, garbage\n",
      "Skipping id: DlXglThZUiPJcuyLLKHV, garbage\n",
      "Skipping id: EBZCFk0aZPVS1SinJwcz, garbage\n",
      "Skipping id: EPBBhoxNvsJ7XdTcj4Qh, garbage\n",
      "Skipping id: FABbgBXx86NQywDLxTQ4, garbage\n",
      "Skipping id: FDT7XFDReB6ZBPkoQwID, garbage\n",
      "Skipping id: FP0UBU9PBzttN2d7YFvZ, garbage\n",
      "Skipping id: G1qq4aXPRbJoge3eLXop, garbage\n",
      "Skipping id: HGuXtX2VtbD3WEKo0kkr, garbage\n",
      "Skipping id: HSGlRuSzUIZ9GvswWelj, garbage\n",
      "Skipping id: Hm6wWR7NyyviL4VTnoIR, garbage\n",
      "Skipping id: HoNSOr041Z9GAlFUwecC, garbage\n",
      "Skipping id: ISOBHbvYDxi04VEFPZWw, garbage\n",
      "Skipping id: ITVfshoCDV8ZOWv9stEz, garbage\n",
      "Skipping id: Iu1De6gLJ0z7eoi7Qb96, garbage\n",
      "Skipping id: J5J8m3KWO5ti1n22kA8S, garbage\n",
      "Skipping id: NQVPkiiV6GRE7Pa2AVWm, garbage\n",
      "Skipping id: NtXdp7ODn4sR3YR2eEW6, garbage\n",
      "Skipping id: O6ZaqitDaFNwoJgIyLr8, garbage\n",
      "Skipping id: OqE5MuCnWDwtAbiinNrn, garbage\n",
      "Skipping id: P05T4keIh8bvF1SKVSSw, garbage\n",
      "Skipping id: QZHmodquL7nvyicTsHI9, garbage\n",
      "Skipping id: RCI4BcA6Bf1kYBk7hUTe, garbage\n",
      "Skipping id: RiYVQs2wMc9D2Q6hfQTt, garbage\n",
      "Skipping id: RwSNxqba5ZhA7Bj5eRsY, garbage\n",
      "Skipping id: U0DUXOSpfqYsgurOyL3L, garbage\n",
      "Skipping id: Uikgs6eO85MRivL0peL6, garbage\n",
      "Skipping id: VU7FcEfu3ebv66OjPam6, garbage\n",
      "Skipping id: VxUBlAJjUKV2wUq7cptD, garbage\n",
      "Skipping id: WFbsW55P5yN47vIzQAiK, garbage\n",
      "Skipping id: WRixt9bnIfzQ6xvgkI2b, garbage\n",
      "Skipping id: WZ2uSJ9YOgp29qJfg57i, garbage\n",
      "Skipping id: XIPyRwJloK6ZdAdTHOeu, garbage\n",
      "Skipping id: XdaHW0tJ9wIGAos7WoG4, garbage\n",
      "Skipping id: Y2V1ZLMVggTj45u8RmzK, garbage\n",
      "Skipping id: aGeDab69j51SPfrmBWvw, garbage\n",
      "Skipping id: ak1I3JXb3x0v1XhrQuAW, garbage\n",
      "Skipping id: bCW1uPMxDEy6iO3oMRQq, garbage\n",
      "Skipping id: bdf0W2iG9B9rZXbe4xun, garbage\n",
      "Skipping id: cYgeFDyGrepaF6G8ap6i, garbage\n",
      "Skipping id: d9FxG5haHnN2ku7spAng, garbage\n",
      "Skipping id: eyzSmD1AdfKkwOYtfoof, garbage\n",
      "Skipping id: f1ojlkGYgnOI4MLRGC4T, garbage\n",
      "Skipping id: fnprJBuBWAE9HH2LqHV6, garbage\n",
      "Skipping id: fySI8xbjfC5hx7QdHpva, garbage\n",
      "Skipping id: gLDxkmHFL3OFufeje6E8, garbage\n",
      "Skipping id: gUXUeStEjNZwDE1j5l8l, garbage\n",
      "Skipping id: gYsv5IG0NS1vkgdmgL2O, garbage\n",
      "Skipping id: hDybtBa855kDWy9l655v, garbage\n",
      "Skipping id: hoguvPcBPEnK4QbuXIp1, garbage\n",
      "Skipping id: i58xevUa8NEItnlX0Jzk, garbage\n",
      "Skipping id: idy5E0oY8I1HevGY7Dlk, garbage\n",
      "Skipping id: jkkOtT3PVoKQMmpA947Y, garbage\n",
      "Skipping id: kBa06RaHA2B9Drl3GtOB, garbage\n",
      "Skipping id: kI7uqiRAJkdBP7QTaTZX, garbage\n",
      "Skipping id: l1HjGNlNwpFzTnCsJxJA, garbage\n",
      "Skipping id: lzjy6lscTmTj1gcdIkNl, garbage\n",
      "Skipping id: o7jTZJyRoBxaDxIQwyQg, garbage\n",
      "Skipping id: osfrRcogOlIrEtxp5fpI, garbage\n",
      "Skipping id: pXXGv1o80FIldPiytsN7, garbage\n",
      "Skipping id: pf1tiPMceCwvXLUfKQwY, garbage\n",
      "Skipping id: pp3QqFslfmbgsdl3dOL8, garbage\n",
      "Skipping id: q2CEZCITA0RVSsfTQRP0, garbage\n",
      "Skipping id: qAwPXAX5efpoQt0TLoUu, garbage\n",
      "Skipping id: qXEJ0wGr86cw8CXIgL4z, garbage\n",
      "Skipping id: r5vxgYs9FnZYOxrh2tcj, garbage\n",
      "Skipping id: rdguN3u8mWRuiRXfUKM8, garbage\n",
      "Skipping id: sGpaBOsdY8MZGC87LzJf, garbage\n",
      "Skipping id: svY8fharA04Fga7FAki5, garbage\n",
      "Skipping id: svZxcinrbiybvsbqk3fT, garbage\n",
      "Skipping id: t58QkfO5GsncvNSmtpQi, garbage\n",
      "Skipping id: uKWTLomw35RqEQw8E6nb, garbage\n",
      "Skipping id: uTDyItbdhyWZoKV3Kc23, garbage\n",
      "Skipping id: uY5HtuacEZGzUXY4D4Lm, garbage\n",
      "Skipping id: ukpHKukvcNPlkhPaTjb5, garbage\n",
      "Skipping id: vDiOKT9fzERg9sasNjMA, garbage\n",
      "Skipping id: vPifbxb1aFBty4kTmrAF, garbage\n",
      "Skipping id: vp4TdnJzy1HbfgTBMIGU, garbage\n",
      "Skipping id: wHotBw9c4TWfO840fYIL, garbage\n",
      "Skipping id: wlKKB1Mkm7GZpnEQjp9w, garbage\n",
      "Skipping id: x2bjhNKDk1cIpaVFa9Ru, garbage\n",
      "Skipping id: xfXFHDkShv17nTMG2rk2, garbage\n",
      "Skipping id: yILwvdGuUBtf5h0U1FWM, garbage\n",
      "Skipping id: yYiXnfz1JA3zrikGBffE, garbage\n",
      "Skipping id: zm29pjekcPNeTWbaQn1U, garbage\n",
      "Skipping id: zmyvgX7sjisN5qDAN3y5, garbage\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.024619102478027344,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 53,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25a37faf27d44de687d4400b15d47618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.022808074951171875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 5220781,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b14b9ee2a0e046d5ae1dc2a78b7eeac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/5.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8 omp threads (processes), consider increasing --nb_cores if you have more\n",
      "Launching the whole pipeline 11/03/2022, 14:19:27\n",
      "There are 3059 embeddings of dim 768\n",
      "\tCompute estimated construction time of the index 11/03/2022, 14:19:27\n",
      "\t\t-> Train: 16.7 minutes\n",
      "\t\t-> Add: 0.0 seconds\n",
      "\t\tTotal: 16.7 minutes\n",
      "\t>>> Finished \"Compute estimated construction time of the index\" in 0.0001 secs\n",
      "\tChecking that your have enough memory available to create the index 11/03/2022, 14:19:27\n",
      "11.2MB of memory will be needed to build the index (more might be used if you have more)\n",
      "\t>>> Finished \"Checking that your have enough memory available to create the index\" in 0.0014 secs\n",
      "\tSelecting most promising index types given data characteristics 11/03/2022, 14:19:27\n",
      "\t>>> Finished \"Selecting most promising index types given data characteristics\" in 0.0000 secs\n",
      "\tCreating the index 11/03/2022, 14:19:27\n",
      "\t\t-> Instanciate the index HNSW15 11/03/2022, 14:19:27\n",
      "\t\t>>> Finished \"-> Instanciate the index HNSW15\" in 0.0041 secs\n",
      "The index size will be approximately 9.3MB\n",
      "The memory available for adding the vectors is 7.0GB(total available - used by the index)\n",
      "Will be using at most 1GB of ram for adding\n",
      "\t\t-> Adding the vectors to the index 11/03/2022, 14:19:27\n",
      "Using a batch size of 325520 (memory overhead 953.7MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 220.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t>>> Finished \"-> Adding the vectors to the index\" in 0.1209 secs\n",
      "\t>>> Finished \"Creating the index\" in 0.1310 secs\n",
      "\tComputing best hyperparameters 11/03/2022, 14:19:27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>>> Finished \"Computing best hyperparameters\" in 3.1366 secs\n",
      "The best hyperparameters are: efSearch=1672\n",
      "\tCompute fast metrics 11/03/2022, 14:19:30\n",
      "2000\n",
      "\t>>> Finished \"Compute fast metrics\" in 7.1165 secs\n",
      "\tSaving the index on local disk 11/03/2022, 14:19:38\n",
      "\t>>> Finished \"Saving the index on local disk\" in 0.0180 secs\n",
      "Recap:\n",
      "{'99p_search_speed_ms': 10.088854670000835,\n",
      " 'avg_search_speed_ms': 3.527450741499649,\n",
      " 'compression ratio': 0.9576690276384732,\n",
      " 'index_key': 'HNSW15',\n",
      " 'index_param': 'efSearch=1672',\n",
      " 'nb vectors': 3059,\n",
      " 'reconstruction error %': 0.0,\n",
      " 'size in bytes': 9812626,\n",
      " 'vectors dimension': 768}\n",
      ">>> Finished \"Launching the whole pipeline\" in 10.4504 secs\n"
     ]
    }
   ],
   "source": [
    "from langame.conversation_starters import get_existing_conversation_starters\n",
    "import logging\n",
    "logger = logging.getLogger(\"classification\")\n",
    "memes, index, embeddings_model = get_existing_conversation_starters(\n",
    "    c._firestore_client, logger=logger, confirmed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file_name = f\"../data/fine_tune_fitness_classification_{datetime.date.today().strftime('%d_%m_%Y')}.jsonl\"\n",
    "\n",
    "for e in memes:\n",
    "    with open(out_file_name, \"a+\") as outfile:\n",
    "        json.dump({\n",
    "            \"prompt\": f\"{','.join(e['topics'])} ### {e['content']} ~~~\",\n",
    "            \"completion\": f\" 1\\n\",\n",
    "        }, outfile)\n",
    "        outfile.write('\\n')\n",
    "for e in c._firestore_client.collection(\"deleted_memes\").stream():\n",
    "    # check has \"topics\" and \"content\"\n",
    "    if \"topics\" not in e.to_dict() or \"content\" not in e.to_dict():\n",
    "        continue\n",
    "    with open(out_file_name, \"a+\") as outfile:\n",
    "        json.dump({\n",
    "            \"prompt\": f\"{','.join(e.get('topics'))} ### {e.get('content')} ~~~\",\n",
    "            \"completion\": f\" 0\\n\",\n",
    "        }, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[34;1m\"prompt\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"personal,relationship,relationships,social,big talk,personal growth ### Give eachother four praises and one critique ~~~\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"completion\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\" 1\\n\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n",
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[34;1m\"prompt\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"ice breaker ### When is a time when you know for sure you'll soon have to exchange a nice, meaningful conversation with someone new? ~~~\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"completion\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\" 1\\n\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n",
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[34;1m\"prompt\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"space exploration,space travel ### Do you think humans are the only intelligent life in the universe? ~~~\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"completion\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\" 1\\n\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n",
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[34;1m\"prompt\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"effective altruism ### What is the most effective way you have found to make the world a better place? ~~~\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"completion\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\" 1\\n\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n",
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[34;1m\"prompt\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"ecology ### Have natural disasters gotten worse with the increase in human existence? If so, why? ~~~\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"completion\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\" 1\\n\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!head -5 $out_file_name | jq ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "execute: openai tools fine_tunes.prepare_data -f /Users/louisbeaumont/Documents/langame-worker/notebooks/../data/fine_tune_fitness_classification_18_09_2022.jsonl\n"
     ]
    }
   ],
   "source": [
    "!echo \"execute: openai tools fine_tunes.prepare_data -f $(pwd)/$out_file_name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"prompt\":\"science,sciences,philosophy,physics,physic ### Do you think humans will ever connect all science to a single theory of everything ~~~\",\"completion\":\" 1\\\\n\"}\\n']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(f\"{out_file_name.replace('.jsonl', '')}_prepared_train.jsonl\").readlines(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = openai.File.create(\n",
    "  file=open(f\"{out_file_name.replace('.jsonl', '')}_prepared_train.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "valid_file = openai.File.create(\n",
    "  file=open(f\"{out_file_name.replace('.jsonl', '')}_prepared_valid.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "ft = openai.FineTune.create(\n",
    "    training_file=train_file[\"id\"],\n",
    "    validation_file=valid_file[\"id\"],\n",
    "    model=\"ada\",\n",
    "    # \"On classification tasks, we recommend setting this to false\"\n",
    "    use_packing=False,\n",
    "    # https://beta.openai.com/docs/api-reference/fine-tunes/create#fine-tunes/create-prompt_loss_weight\n",
    "    # classification recommended 0.1 (it's the default value anyway)\n",
    "    prompt_loss_weight=0.1,\n",
    "    compute_classification_metrics=True,\n",
    "    classification_positive_class=\" 1\\n\",\n",
    "    classification_n_classes=2,\n",
    "    suffix=\"fitness-classification\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:07:28\n",
      "event_message: Created fine-tune: ft-5IshDTQ5HIC8Rt8zbbDCBKJY\n",
      "event_created_at: 2022-09-18 16:21:28\n",
      "event_message: Fine-tune costs $0.25\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:21:30\n",
      "event_message: Fine-tune started\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:26:03\n",
      "event_message: Completed epoch 1/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:30:38\n",
      "event_message: Completed epoch 2/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:35:12\n",
      "event_message: Completed epoch 3/4\n",
      "event_created_at: 2022-09-18 16:39:42\n",
      "event_message: Completed epoch 4/4\n",
      "event_created_at: 2022-09-18 16:39:42\n",
      "event_message: Completed epoch 4/4\n",
      "event_created_at: 2022-09-18 16:39:42\n",
      "event_message: Completed epoch 4/4\n",
      "event_created_at: 2022-09-18 16:39:42\n",
      "event_message: Completed epoch 4/4\n",
      "event_created_at: 2022-09-18 16:39:42\n",
      "event_message: Completed epoch 4/4\n",
      "event_created_at: 2022-09-18 16:39:42\n",
      "event_message: Completed epoch 4/4\n",
      "event_created_at: 2022-09-18 16:39:42\n",
      "event_message: Completed epoch 4/4\n",
      "event_created_at: 2022-09-18 16:39:42\n",
      "event_message: Completed epoch 4/4\n",
      "event_created_at: 2022-09-18 16:39:42\n",
      "event_message: Completed epoch 4/4\n",
      "event_created_at: 2022-09-18 16:39:42\n",
      "event_message: Completed epoch 4/4\n",
      "event_created_at: 2022-09-18 16:39:42\n",
      "event_message: Completed epoch 4/4\n",
      "event_created_at: 2022-09-18 16:39:42\n",
      "event_message: Completed epoch 4/4\n",
      "event_created_at: 2022-09-18 16:40:20\n",
      "event_message: Fine-tune succeeded\n",
      "fine-tuning done ada:ft-personal:fitness-classification-2022-09-18-14-40-19\n",
      "paid 0.25\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/louisbeaumont/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlouis030195\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/louisbeaumont/Documents/langame-worker/notebooks/wandb/run-20220918_164025-ft-5IshDTQ5HIC8Rt8zbbDCBKJY</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/louis030195/langame-classification/runs/ft-5IshDTQ5HIC8Rt8zbbDCBKJY\" target=\"_blank\">ft-5IshDTQ5HIC8Rt8zbbDCBKJY</a></strong> to <a href=\"https://wandb.ai/louis030195/langame-classification\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>classification/accuracy</td><td>▁█▆▆</td></tr><tr><td>classification/auprc</td><td>▁▇██</td></tr><tr><td>classification/auroc</td><td>▁███</td></tr><tr><td>classification/f1.0</td><td>▁█▇█</td></tr><tr><td>classification/precision</td><td>▁█▅▄</td></tr><tr><td>classification/recall</td><td>▅▁▅█</td></tr><tr><td>elapsed_examples</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>elapsed_tokens</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_loss</td><td>▆▅█▅▄█▇█▃▂▃▂▃▇▄▆▄▆▆▆▃▃▁▂▃▅▁▄▃▇▁▁▂▃▃▄▄▄▄▁</td></tr><tr><td>training_sequence_accuracy</td><td>▁▁▃▁▁▆▃█▆▆▃█▃█▆█▆████▆███▆██▁▃██▃▆██▆█▆█</td></tr><tr><td>training_token_accuracy</td><td>▁▁▃▁▁▆▃█▆▆▃█▃█▆█▆████▆███▆██▁▃██▃▆██▆█▆█</td></tr><tr><td>validation_loss</td><td>▆█▆▃▅▃▁▃▇▅▃▃▂▃▅▄▃▇▃▄▄▄▁▂▂▂█▁▄▂▁▂▆▁▃▅▄▁▄▂</td></tr><tr><td>validation_sequence_accuracy</td><td>▁█▆▆▇█▆▇▃▇█▃▇█▇▅▇▇▅▇▆▆▅▇█▆▆▇█▇▇▆▆▇██████</td></tr><tr><td>validation_token_accuracy</td><td>▁█▆▆▇█▆▇▃▇█▃▇█▇▅▇▇▅▇▆▆▅▇█▆▆▇█▇▇▆▆▇██████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>classification/accuracy</td><td>0.854</td></tr><tr><td>classification/auprc</td><td>0.92285</td></tr><tr><td>classification/auroc</td><td>0.94355</td></tr><tr><td>classification/f1.0</td><td>0.83371</td></tr><tr><td>classification/precision</td><td>0.81514</td></tr><tr><td>classification/recall</td><td>0.85315</td></tr><tr><td>elapsed_examples</td><td>21400.0</td></tr><tr><td>elapsed_tokens</td><td>1129560.0</td></tr><tr><td>fine_tuned_model</td><td>ada:ft-personal:fitn...</td></tr><tr><td>status</td><td>succeeded</td></tr><tr><td>training_loss</td><td>0.08284</td></tr><tr><td>training_sequence_accuracy</td><td>1.0</td></tr><tr><td>training_token_accuracy</td><td>1.0</td></tr><tr><td>validation_loss</td><td>0.09989</td></tr><tr><td>validation_sequence_accuracy</td><td>1.0</td></tr><tr><td>validation_token_accuracy</td><td>1.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">ft-5IshDTQ5HIC8Rt8zbbDCBKJY</strong>: <a href=\"https://wandb.ai/louis030195/langame-classification/runs/ft-5IshDTQ5HIC8Rt8zbbDCBKJY\" target=\"_blank\">https://wandb.ai/louis030195/langame-classification/runs/ft-5IshDTQ5HIC8Rt8zbbDCBKJY</a><br/>Synced 4 W&B file(s), 0 media file(s), 5 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220918_164025-ft-5IshDTQ5HIC8Rt8zbbDCBKJY/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'🎉 wandb sync completed successfully'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "cost = \"\"\n",
    "created_at = \"\"\n",
    "event_message = \"\"\n",
    "while True:\n",
    "    time.sleep(3)\n",
    "    results = openai.FineTune.retrieve(\n",
    "        ft[\"id\"],\n",
    "    )\n",
    "    if event_message == results[\"events\"][-1][\"message\"]:\n",
    "        continue\n",
    "    # \"created_at\" timestamp to human readable\n",
    "    created_at = time.strftime(\n",
    "        \"%Y-%m-%d %H:%M:%S\", time.localtime(results[\"events\"][-1][\"created_at\"]))\n",
    "    print(\"event_created_at:\", created_at)\n",
    "    print(\"event_message:\", results[\"events\"][-1][\"message\"])\n",
    "    if \"$\" in results[\"events\"][-1][\"message\"]:\n",
    "        cost = results[\"events\"][-1][\"message\"].split(\"$\")[1]\n",
    "    if results[\"fine_tuned_model\"] is not None:\n",
    "        print(\"fine-tuning done\", results[\"fine_tuned_model\"])\n",
    "        print(\"paid\", cost)\n",
    "        break\n",
    "from openai.wandb_logger import WandbLogger\n",
    "import wandb\n",
    "import re\n",
    "values = open(\"../.env.production\", \"r\").read()\n",
    "key = re.findall(r\"WANDB_KEY=\\\"(.*)\\\"\", values)[0]\n",
    "wandb.login(key=key, relogin=True)\n",
    "WandbLogger.sync(\n",
    "    id=ft[\"id\"],\n",
    "    project=\"langame-classification\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRLX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping id: 0ZcPZDcvuqgialj2wHpL, garbage\n",
      "Skipping id: 0m2ltwf7wkxLzcHnFkQE, garbage\n",
      "Skipping id: 1YWuwIp4srffwPNq7Aw9, garbage\n",
      "Skipping id: 4XlXSjPhH6QSGZDzwIQe, garbage\n",
      "Skipping id: 4sIjdMJr0exliT9yfvuL, garbage\n",
      "Skipping id: 5MAMBAYySyu3lKrR3zLt, garbage\n",
      "Skipping id: 6jNOw0e7VA6lHWddtqMy, garbage\n",
      "Skipping id: 7yHXwGPlHpopQAHU3xf1, garbage\n",
      "Skipping id: 8e9jVhvFpUIzj7ocNv06, garbage\n",
      "Skipping id: 8tzYKdv2Hkwxsm8eNQgj, garbage\n",
      "Skipping id: 9NdMRn8C285Eu3k4PdsS, garbage\n",
      "Skipping id: 9ZfEEkrB3cjLooW6YZM2, garbage\n",
      "Skipping id: B8hRGbGGOweqEyxuuplE, garbage\n",
      "Skipping id: BH9ZtGe1g6q3KmK7oV2f, garbage\n",
      "Skipping id: BZjXMPPKlmcqXG23i0mI, garbage\n",
      "Skipping id: BwSIpHO7X8N1fhf5zTyr, garbage\n",
      "Skipping id: C0yrKrOeX5kQKwEgyrJS, garbage\n",
      "Skipping id: CYsOWtp1ed2XDeplB5O5, garbage\n",
      "Skipping id: D8A4QcAlqidVArVPDKGP, garbage\n",
      "Skipping id: DhuuWoHv7oGKSA5agAzZ, garbage\n",
      "Skipping id: DjuEuISOCOG6R2D7ZlSy, garbage\n",
      "Skipping id: DlXglThZUiPJcuyLLKHV, garbage\n",
      "Skipping id: EBZCFk0aZPVS1SinJwcz, garbage\n",
      "Skipping id: EPBBhoxNvsJ7XdTcj4Qh, garbage\n",
      "Skipping id: FABbgBXx86NQywDLxTQ4, garbage\n",
      "Skipping id: FDT7XFDReB6ZBPkoQwID, garbage\n",
      "Skipping id: FP0UBU9PBzttN2d7YFvZ, garbage\n",
      "Skipping id: G1qq4aXPRbJoge3eLXop, garbage\n",
      "Skipping id: HGuXtX2VtbD3WEKo0kkr, garbage\n",
      "Skipping id: HSGlRuSzUIZ9GvswWelj, garbage\n",
      "Skipping id: Hm6wWR7NyyviL4VTnoIR, garbage\n",
      "Skipping id: HoNSOr041Z9GAlFUwecC, garbage\n",
      "Skipping id: ISOBHbvYDxi04VEFPZWw, garbage\n",
      "Skipping id: ITVfshoCDV8ZOWv9stEz, garbage\n",
      "Skipping id: Iu1De6gLJ0z7eoi7Qb96, garbage\n",
      "Skipping id: J5J8m3KWO5ti1n22kA8S, garbage\n",
      "Skipping id: NQVPkiiV6GRE7Pa2AVWm, garbage\n",
      "Skipping id: NtXdp7ODn4sR3YR2eEW6, garbage\n",
      "Skipping id: O6ZaqitDaFNwoJgIyLr8, garbage\n",
      "Skipping id: OqE5MuCnWDwtAbiinNrn, garbage\n",
      "Skipping id: P05T4keIh8bvF1SKVSSw, garbage\n",
      "Skipping id: QZHmodquL7nvyicTsHI9, garbage\n",
      "Skipping id: RCI4BcA6Bf1kYBk7hUTe, garbage\n",
      "Skipping id: RiYVQs2wMc9D2Q6hfQTt, garbage\n",
      "Skipping id: RwSNxqba5ZhA7Bj5eRsY, garbage\n",
      "Skipping id: U0DUXOSpfqYsgurOyL3L, garbage\n",
      "Skipping id: Uikgs6eO85MRivL0peL6, garbage\n",
      "Skipping id: VU7FcEfu3ebv66OjPam6, garbage\n",
      "Skipping id: VxUBlAJjUKV2wUq7cptD, garbage\n",
      "Skipping id: WFbsW55P5yN47vIzQAiK, garbage\n",
      "Skipping id: WRixt9bnIfzQ6xvgkI2b, garbage\n",
      "Skipping id: WZ2uSJ9YOgp29qJfg57i, garbage\n",
      "Skipping id: XIPyRwJloK6ZdAdTHOeu, garbage\n",
      "Skipping id: XdaHW0tJ9wIGAos7WoG4, garbage\n",
      "Skipping id: Y2V1ZLMVggTj45u8RmzK, garbage\n",
      "Skipping id: aGeDab69j51SPfrmBWvw, garbage\n",
      "Skipping id: ak1I3JXb3x0v1XhrQuAW, garbage\n",
      "Skipping id: bCW1uPMxDEy6iO3oMRQq, garbage\n",
      "Skipping id: bdf0W2iG9B9rZXbe4xun, garbage\n",
      "Skipping id: cYgeFDyGrepaF6G8ap6i, garbage\n",
      "Skipping id: d9FxG5haHnN2ku7spAng, garbage\n",
      "Skipping id: eyzSmD1AdfKkwOYtfoof, garbage\n",
      "Skipping id: f1ojlkGYgnOI4MLRGC4T, garbage\n",
      "Skipping id: fnprJBuBWAE9HH2LqHV6, garbage\n",
      "Skipping id: fySI8xbjfC5hx7QdHpva, garbage\n",
      "Skipping id: gLDxkmHFL3OFufeje6E8, garbage\n",
      "Skipping id: gUXUeStEjNZwDE1j5l8l, garbage\n",
      "Skipping id: gYsv5IG0NS1vkgdmgL2O, garbage\n",
      "Skipping id: hDybtBa855kDWy9l655v, garbage\n",
      "Skipping id: hoguvPcBPEnK4QbuXIp1, garbage\n",
      "Skipping id: i58xevUa8NEItnlX0Jzk, garbage\n",
      "Skipping id: idy5E0oY8I1HevGY7Dlk, garbage\n",
      "Skipping id: jkkOtT3PVoKQMmpA947Y, garbage\n",
      "Skipping id: kBa06RaHA2B9Drl3GtOB, garbage\n",
      "Skipping id: kI7uqiRAJkdBP7QTaTZX, garbage\n",
      "Skipping id: l1HjGNlNwpFzTnCsJxJA, garbage\n",
      "Skipping id: lzjy6lscTmTj1gcdIkNl, garbage\n",
      "Skipping id: o7jTZJyRoBxaDxIQwyQg, garbage\n",
      "Skipping id: osfrRcogOlIrEtxp5fpI, garbage\n",
      "Skipping id: pXXGv1o80FIldPiytsN7, garbage\n",
      "Skipping id: pf1tiPMceCwvXLUfKQwY, garbage\n",
      "Skipping id: pp3QqFslfmbgsdl3dOL8, garbage\n",
      "Skipping id: q2CEZCITA0RVSsfTQRP0, garbage\n",
      "Skipping id: qAwPXAX5efpoQt0TLoUu, garbage\n",
      "Skipping id: qXEJ0wGr86cw8CXIgL4z, garbage\n",
      "Skipping id: r5vxgYs9FnZYOxrh2tcj, garbage\n",
      "Skipping id: rdguN3u8mWRuiRXfUKM8, garbage\n",
      "Skipping id: sGpaBOsdY8MZGC87LzJf, garbage\n",
      "Skipping id: svY8fharA04Fga7FAki5, garbage\n",
      "Skipping id: svZxcinrbiybvsbqk3fT, garbage\n",
      "Skipping id: t58QkfO5GsncvNSmtpQi, garbage\n",
      "Skipping id: uKWTLomw35RqEQw8E6nb, garbage\n",
      "Skipping id: uTDyItbdhyWZoKV3Kc23, garbage\n",
      "Skipping id: uY5HtuacEZGzUXY4D4Lm, garbage\n",
      "Skipping id: ukpHKukvcNPlkhPaTjb5, garbage\n",
      "Skipping id: vDiOKT9fzERg9sasNjMA, garbage\n",
      "Skipping id: vPifbxb1aFBty4kTmrAF, garbage\n",
      "Skipping id: vp4TdnJzy1HbfgTBMIGU, garbage\n",
      "Skipping id: wHotBw9c4TWfO840fYIL, garbage\n",
      "Skipping id: wlKKB1Mkm7GZpnEQjp9w, garbage\n",
      "Skipping id: x2bjhNKDk1cIpaVFa9Ru, garbage\n",
      "Skipping id: xfXFHDkShv17nTMG2rk2, garbage\n",
      "Skipping id: yILwvdGuUBtf5h0U1FWM, garbage\n",
      "Skipping id: yYiXnfz1JA3zrikGBffE, garbage\n",
      "Skipping id: zm29pjekcPNeTWbaQn1U, garbage\n",
      "Skipping id: zmyvgX7sjisN5qDAN3y5, garbage\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8 omp threads (processes), consider increasing --nb_cores if you have more\n",
      "Launching the whole pipeline 11/03/2022, 15:40:46\n",
      "There are 3059 embeddings of dim 768\n",
      "\tCompute estimated construction time of the index 11/03/2022, 15:40:46\n",
      "\t\t-> Train: 16.7 minutes\n",
      "\t\t-> Add: 0.0 seconds\n",
      "\t\tTotal: 16.7 minutes\n",
      "\t>>> Finished \"Compute estimated construction time of the index\" in 0.0001 secs\n",
      "\tChecking that your have enough memory available to create the index 11/03/2022, 15:40:46\n",
      "11.2MB of memory will be needed to build the index (more might be used if you have more)\n",
      "\t>>> Finished \"Checking that your have enough memory available to create the index\" in 0.0018 secs\n",
      "\tSelecting most promising index types given data characteristics 11/03/2022, 15:40:46\n",
      "\t>>> Finished \"Selecting most promising index types given data characteristics\" in 0.0000 secs\n",
      "\tCreating the index 11/03/2022, 15:40:46\n",
      "\t\t-> Instanciate the index HNSW15 11/03/2022, 15:40:46\n",
      "\t\t>>> Finished \"-> Instanciate the index HNSW15\" in 0.0036 secs\n",
      "The index size will be approximately 9.3MB\n",
      "The memory available for adding the vectors is 7.0GB(total available - used by the index)\n",
      "Will be using at most 1GB of ram for adding\n",
      "\t\t-> Adding the vectors to the index 11/03/2022, 15:40:46\n",
      "Using a batch size of 325520 (memory overhead 953.7MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 236.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t>>> Finished \"-> Adding the vectors to the index\" in 0.1104 secs\n",
      "\t>>> Finished \"Creating the index\" in 0.1150 secs\n",
      "\tComputing best hyperparameters 11/03/2022, 15:40:46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>>> Finished \"Computing best hyperparameters\" in 3.9634 secs\n",
      "The best hyperparameters are: efSearch=15386\n",
      "\tCompute fast metrics 11/03/2022, 15:40:50\n",
      "1406\n",
      "\t>>> Finished \"Compute fast metrics\" in 10.0660 secs\n",
      "\tSaving the index on local disk 11/03/2022, 15:41:00\n",
      "\t>>> Finished \"Saving the index on local disk\" in 0.0119 secs\n",
      "Recap:\n",
      "{'99p_search_speed_ms': 10.4906702000001,\n",
      " 'avg_search_speed_ms': 7.112143985063913,\n",
      " 'compression ratio': 0.9576690276384732,\n",
      " 'index_key': 'HNSW15',\n",
      " 'index_param': 'efSearch=15386',\n",
      " 'nb vectors': 3059,\n",
      " 'reconstruction error %': 0.0,\n",
      " 'size in bytes': 9812626,\n",
      " 'vectors dimension': 768}\n",
      ">>> Finished \"Launching the whole pipeline\" in 14.2078 secs\n"
     ]
    }
   ],
   "source": [
    "from langame.conversation_starters import get_existing_conversation_starters\n",
    "import logging\n",
    "logger = logging.getLogger(\"classification\")\n",
    "memes, index, embeddings_model = get_existing_conversation_starters(\n",
    "    c._firestore_client, logger=logger, confirmed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom model topic inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping id: 4XlXSjPhH6QSGZDzwIQe, garbage\n",
      "Skipping id: 4sIjdMJr0exliT9yfvuL, garbage\n",
      "Skipping id: 6jNOw0e7VA6lHWddtqMy, garbage\n",
      "Skipping id: 9ZfEEkrB3cjLooW6YZM2, garbage\n",
      "Skipping id: B8hRGbGGOweqEyxuuplE, garbage\n",
      "Skipping id: CYsOWtp1ed2XDeplB5O5, garbage\n",
      "Skipping id: DlXglThZUiPJcuyLLKHV, garbage\n",
      "Skipping id: EBZCFk0aZPVS1SinJwcz, garbage\n",
      "Skipping id: FP0UBU9PBzttN2d7YFvZ, garbage\n",
      "Skipping id: ISOBHbvYDxi04VEFPZWw, garbage\n",
      "Skipping id: Iu1De6gLJ0z7eoi7Qb96, garbage\n",
      "Skipping id: NtXdp7ODn4sR3YR2eEW6, garbage\n",
      "Skipping id: P05T4keIh8bvF1SKVSSw, garbage\n",
      "Skipping id: RCI4BcA6Bf1kYBk7hUTe, garbage\n",
      "Skipping id: U0DUXOSpfqYsgurOyL3L, garbage\n",
      "Skipping id: Uikgs6eO85MRivL0peL6, garbage\n",
      "Skipping id: XIPyRwJloK6ZdAdTHOeu, garbage\n",
      "Skipping id: XdaHW0tJ9wIGAos7WoG4, garbage\n",
      "Skipping id: Y2V1ZLMVggTj45u8RmzK, garbage\n",
      "Skipping id: bCW1uPMxDEy6iO3oMRQq, garbage\n",
      "Skipping id: bdf0W2iG9B9rZXbe4xun, garbage\n",
      "Skipping id: d9FxG5haHnN2ku7spAng, garbage\n",
      "Skipping id: eyzSmD1AdfKkwOYtfoof, garbage\n",
      "Skipping id: f1ojlkGYgnOI4MLRGC4T, garbage\n",
      "Skipping id: fnprJBuBWAE9HH2LqHV6, garbage\n",
      "Skipping id: fySI8xbjfC5hx7QdHpva, garbage\n",
      "Skipping id: gLDxkmHFL3OFufeje6E8, garbage\n",
      "Skipping id: gUXUeStEjNZwDE1j5l8l, garbage\n",
      "Skipping id: gYsv5IG0NS1vkgdmgL2O, garbage\n",
      "Skipping id: hDybtBa855kDWy9l655v, garbage\n",
      "Skipping id: hoguvPcBPEnK4QbuXIp1, garbage\n",
      "Skipping id: i58xevUa8NEItnlX0Jzk, garbage\n",
      "Skipping id: idy5E0oY8I1HevGY7Dlk, garbage\n",
      "Skipping id: jkkOtT3PVoKQMmpA947Y, garbage\n",
      "Skipping id: kI7uqiRAJkdBP7QTaTZX, garbage\n",
      "Skipping id: l1HjGNlNwpFzTnCsJxJA, garbage\n",
      "Skipping id: lzjy6lscTmTj1gcdIkNl, garbage\n",
      "Skipping id: o7jTZJyRoBxaDxIQwyQg, garbage\n",
      "Skipping id: osfrRcogOlIrEtxp5fpI, garbage\n",
      "Skipping id: pXXGv1o80FIldPiytsN7, garbage\n",
      "Skipping id: pf1tiPMceCwvXLUfKQwY, garbage\n",
      "Skipping id: q2CEZCITA0RVSsfTQRP0, garbage\n",
      "Skipping id: qAwPXAX5efpoQt0TLoUu, garbage\n",
      "Skipping id: qXEJ0wGr86cw8CXIgL4z, garbage\n",
      "Skipping id: r5vxgYs9FnZYOxrh2tcj, garbage\n",
      "Skipping id: rdguN3u8mWRuiRXfUKM8, garbage\n",
      "Skipping id: svY8fharA04Fga7FAki5, garbage\n",
      "Skipping id: svZxcinrbiybvsbqk3fT, garbage\n",
      "Skipping id: t58QkfO5GsncvNSmtpQi, garbage\n",
      "Skipping id: uTDyItbdhyWZoKV3Kc23, garbage\n",
      "Skipping id: uY5HtuacEZGzUXY4D4Lm, garbage\n",
      "Skipping id: ukpHKukvcNPlkhPaTjb5, garbage\n",
      "Skipping id: vDiOKT9fzERg9sasNjMA, garbage\n",
      "Skipping id: vPifbxb1aFBty4kTmrAF, garbage\n",
      "Skipping id: vp4TdnJzy1HbfgTBMIGU, garbage\n",
      "Skipping id: wHotBw9c4TWfO840fYIL, garbage\n",
      "Skipping id: wlKKB1Mkm7GZpnEQjp9w, garbage\n",
      "Skipping id: xfXFHDkShv17nTMG2rk2, garbage\n",
      "Skipping id: yILwvdGuUBtf5h0U1FWM, garbage\n",
      "Skipping id: yYiXnfz1JA3zrikGBffE, garbage\n",
      "Skipping id: zm29pjekcPNeTWbaQn1U, garbage\n"
     ]
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "Column 1 named validation expected length 1956 but got length 490",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu@91.175.184.114:49512/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb#ch0000013vscode-remote?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlogging\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu@91.175.184.114:49512/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb#ch0000013vscode-remote?line=2'>3</a>\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m\"\u001b[39m\u001b[39mclassification\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu@91.175.184.114:49512/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb#ch0000013vscode-remote?line=3'>4</a>\u001b[0m memes, index, embeddings_model \u001b[39m=\u001b[39m get_existing_conversation_starters(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu@91.175.184.114:49512/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb#ch0000013vscode-remote?line=4'>5</a>\u001b[0m     c\u001b[39m.\u001b[39;49m_firestore_client, logger\u001b[39m=\u001b[39;49mlogger, confirmed\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, push_to_hub\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/langame-worker/langame/conversation_starters.py:70\u001b[0m, in \u001b[0;36mget_existing_conversation_starters\u001b[0;34m(client, logger, use_gpu, limit, batch_embeddings_size, confirmed, push_to_hub)\u001b[0m\n\u001b[1;32m     65\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m     66\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(existing_conversation_starters)\u001b[39m}\u001b[39;00m\u001b[39m existing conversation starters\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m     )\n\u001b[1;32m     68\u001b[0m \u001b[39mif\u001b[39;00m push_to_hub:\n\u001b[1;32m     69\u001b[0m     \u001b[39m# TODO: clean dataset no id...\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     dataset \u001b[39m=\u001b[39m Dataset\u001b[39m.\u001b[39;49mfrom_dict({\n\u001b[1;32m     71\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m: existing_conversation_starters[:\u001b[39mint\u001b[39;49m(\u001b[39mlen\u001b[39;49m(existing_conversation_starters) \u001b[39m*\u001b[39;49m \u001b[39m0.8\u001b[39;49m)],\n\u001b[1;32m     72\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mvalidation\u001b[39;49m\u001b[39m\"\u001b[39;49m: existing_conversation_starters[\u001b[39mint\u001b[39;49m(\u001b[39mlen\u001b[39;49m(existing_conversation_starters) \u001b[39m*\u001b[39;49m \u001b[39m0.8\u001b[39;49m):],\n\u001b[1;32m     73\u001b[0m     })\n\u001b[1;32m     74\u001b[0m     dataset\u001b[39m.\u001b[39mpush_to_hub(\u001b[39m\"\u001b[39m\u001b[39mLangame/starter\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     76\u001b[0m \u001b[39mif\u001b[39;00m logger:\n",
      "File \u001b[0;32m~/langame-worker/env/lib/python3.8/site-packages/datasets/arrow_dataset.py:859\u001b[0m, in \u001b[0;36mDataset.from_dict\u001b[0;34m(cls, mapping, features, info, split)\u001b[0m\n\u001b[1;32m    854\u001b[0m     mapping \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mencode_batch(mapping)\n\u001b[1;32m    855\u001b[0m mapping \u001b[39m=\u001b[39m {\n\u001b[1;32m    856\u001b[0m     col: OptimizedTypedSequence(data, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39mfeatures[col] \u001b[39mif\u001b[39;00m features \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, col\u001b[39m=\u001b[39mcol)\n\u001b[1;32m    857\u001b[0m     \u001b[39mfor\u001b[39;00m col, data \u001b[39min\u001b[39;00m mapping\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    858\u001b[0m }\n\u001b[0;32m--> 859\u001b[0m pa_table \u001b[39m=\u001b[39m InMemoryTable\u001b[39m.\u001b[39;49mfrom_pydict(mapping\u001b[39m=\u001b[39;49mmapping)\n\u001b[1;32m    860\u001b[0m \u001b[39mif\u001b[39;00m info\u001b[39m.\u001b[39mfeatures \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    861\u001b[0m     info\u001b[39m.\u001b[39mfeatures \u001b[39m=\u001b[39m Features({col: ts\u001b[39m.\u001b[39mget_inferred_type() \u001b[39mfor\u001b[39;00m col, ts \u001b[39min\u001b[39;00m mapping\u001b[39m.\u001b[39mitems()})\n",
      "File \u001b[0;32m~/langame-worker/env/lib/python3.8/site-packages/datasets/table.py:750\u001b[0m, in \u001b[0;36mInMemoryTable.from_pydict\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pydict\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    736\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \u001b[39m    Construct a Table from Arrow arrays or columns\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[39m        :class:`datasets.table.Table`:\u001b[39;00m\n\u001b[1;32m    749\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(pa\u001b[39m.\u001b[39;49mTable\u001b[39m.\u001b[39;49mfrom_pydict(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[0;32m~/langame-worker/env/lib/python3.8/site-packages/pyarrow/table.pxi:3625\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_pydict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/langame-worker/env/lib/python3.8/site-packages/pyarrow/table.pxi:5151\u001b[0m, in \u001b[0;36mpyarrow.lib._from_pydict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/langame-worker/env/lib/python3.8/site-packages/pyarrow/table.pxi:3574\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_arrays\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/langame-worker/env/lib/python3.8/site-packages/pyarrow/table.pxi:2793\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.validate\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/langame-worker/env/lib/python3.8/site-packages/pyarrow/error.pxi:100\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: Column 1 named validation expected length 1956 but got length 490"
     ]
    }
   ],
   "source": [
    "\n",
    "logger = logging.getLogger(\"classification\")\n",
    "memes, index, embeddings_model = get_existing_conversation_starters(\n",
    "    c._firestore_client, logger=logger, confirmed=True, push_to_hub=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out_file_name = f\"../data/fine_tune_topic_classification_{datetime.date.today().strftime('%d_%m_%Y')}.txt\"\n",
    "\n",
    "# make a dict of {\"TOPIC\": int}\n",
    "topic_to_int = {}\n",
    "for e in memes:\n",
    "    for topic in e['topics']:\n",
    "        if topic not in topic_to_int:\n",
    "            topic_to_int[topic] = len(topic_to_int)\n",
    "try:\n",
    "    os.remove(out_file_name)\n",
    "except:\n",
    "    pass\n",
    "for e in memes:\n",
    "    with open(out_file_name, \"a+\") as outfile:\n",
    "        if len(e['topics']) == 0: continue\n",
    "        outfile.write(f\"{e['content']} ### {','.join(map(str, e['topics']))}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "When is a time when you know for sure you'll soon have to exchange a nice, meaningful conversation with someone new? ### ice breaker\n",
      "Do you think humans are the only intelligent life in the universe? ### space exploration,space travel\n",
      "Have natural disasters gotten worse with the increase in human existence? If so, why? ### ecology\n"
     ]
    }
   ],
   "source": [
    "!head $out_file_name -n3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-f18bc8eb4d57408a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /home/gpu/.cache/huggingface/datasets/text/default-f18bc8eb4d57408a/0.0.0/acc32f2f2ef863c93c2f30c52f7df6cc9053a1c2230b8d7da0d210404683ca08...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abea66a96a9348018341d47d014db85b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23866c0d97f4db28c9edcde9b66fa56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34558ed324c46c0ba8c4c8ff8f1d043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e63002f27640caa02808f37a3c71cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /home/gpu/.cache/huggingface/datasets/text/default-f18bc8eb4d57408a/0.0.0/acc32f2f2ef863c93c2f30c52f7df6cc9053a1c2230b8d7da0d210404683ca08. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c765a09d7394222bdaf643ac589a8fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# split the file into train.txt and eval.txt\n",
    "with open(out_file_name, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    train_lines = lines[:int(len(lines) * 0.8)]\n",
    "    eval_lines = lines[int(len(lines) * 0.8):]\n",
    "    with open(\"train.txt\", \"w\") as f:\n",
    "        f.writelines(train_lines)\n",
    "    with open(\"eval.txt\", \"w\") as f:\n",
    "        f.writelines(eval_lines)\n",
    "dataset = load_dataset('text', data_files={\"train\": \"train.txt\", \"eval\": \"eval.txt\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /home/gpu/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"distilgpt2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 6,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilgpt2/resolve/main/pytorch_model.bin from cache at /home/gpu/.cache/huggingface/transformers/43a212e83e76bcb07f45be584cf100676bdbbbe9c13f9e5c1c050049143a832f.a83d881ec4d624fd4b5826dd026e315246c48c67504ff91c0500570e291a54ba\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /home/gpu/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"distilgpt2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 6,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilgpt2/resolve/main/vocab.json from cache at /home/gpu/.cache/huggingface/transformers/55051ac97dcc32f0a736d21a32a4d42b0d9b90f117ca7c38e65038b04bd5c3f5.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "loading file https://huggingface.co/distilgpt2/resolve/main/merges.txt from cache at /home/gpu/.cache/huggingface/transformers/9dfb299b74cdf7601ba7cd3a8073dbdac351caec0ed7ab5849b098b3c8ae3d57.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/distilgpt2/resolve/main/tokenizer.json from cache at /home/gpu/.cache/huggingface/transformers/accb287b5a5396b2597382916b6cc939fdab1366e89475a92338d3971b3d02b7.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "loading file https://huggingface.co/distilgpt2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilgpt2/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilgpt2/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /home/gpu/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"distilgpt2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 6,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c2279135d24003af22082f628aa6c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c89994fc1f654404865cb974496f6637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dea3f0254ff40f79a6b84486de746f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dba50a5b73548f79e7e06ffafddc355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121e631c9e83415f9c433641aaf2f509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b123d751b5a14c9b8319e6131368b73e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d113ef2f505843d2a28bcb72aed56641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4690a6a9c46845e293bff3aafd3eaca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd070cd3f90049378bf94b396a5abe63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69132126a9874917b684b18dddad976f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b52933da44f4bd7a2f90902251d7bc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6964211bf73b4cf1b33271d90eb3b82f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36850bba3acc478a8dcdfe150506646b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28bcc301eb9d43e4816624adcbb014e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3331766463145cc9b5bb4aeb1c5f716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f3e17770d794c78968e38c3fbfa38b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "# tokenizer.eos_token = 198\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "def preprocess_function(x):\n",
    "    return tokenizer([e + \"\\n\" for e in x[\"text\"]])\n",
    "tokenized_eli5 = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=[\"text\"],\n",
    ")\n",
    "block_size = 128\n",
    "\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "lm_datasets = tokenized_eli5.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")\n",
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/gpu/langame-worker/env/lib/python3.8/site-packages/transformers/training_args.py:1186: FutureWarning: `--push_to_hub_model_id` and `--push_to_hub_organization` are deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_model_id` instead and pass the full repo name to this argument (in this case Langame/distilgpt2-starter-classification).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Tried to clone a repository in a non-empty folder that isn't a git repository. If you really want to do this, do it manually:\ngit init && git remote add origin && git pull origin main\n or clone repo to a new folder and move your existing files there afterwards.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu@91.175.184.114:49512/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb#ch0000019vscode-remote?line=0'>1</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu@91.175.184.114:49512/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb#ch0000019vscode-remote?line=1'>2</a>\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./results\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu@91.175.184.114:49512/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb#ch0000019vscode-remote?line=2'>3</a>\u001b[0m     evaluation_strategy\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu@91.175.184.114:49512/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb#ch0000019vscode-remote?line=9'>10</a>\u001b[0m     push_to_hub\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu@91.175.184.114:49512/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb#ch0000019vscode-remote?line=10'>11</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu@91.175.184.114:49512/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb#ch0000019vscode-remote?line=12'>13</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu@91.175.184.114:49512/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb#ch0000019vscode-remote?line=13'>14</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu@91.175.184.114:49512/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb#ch0000019vscode-remote?line=14'>15</a>\u001b[0m     args\u001b[39m=\u001b[39;49mtraining_args,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu@91.175.184.114:49512/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb#ch0000019vscode-remote?line=15'>16</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39;49mlm_datasets[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu@91.175.184.114:49512/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb#ch0000019vscode-remote?line=16'>17</a>\u001b[0m     eval_dataset\u001b[39m=\u001b[39;49mlm_datasets[\u001b[39m\"\u001b[39;49m\u001b[39meval\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu@91.175.184.114:49512/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb#ch0000019vscode-remote?line=17'>18</a>\u001b[0m     \u001b[39m# data_collator=data_collator,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu@91.175.184.114:49512/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb#ch0000019vscode-remote?line=18'>19</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu@91.175.184.114:49512/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb#ch0000019vscode-remote?line=20'>21</a>\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/langame-worker/env/lib/python3.8/site-packages/transformers/trainer.py:464\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39m# Create clone of distant repo and output directory if needed\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpush_to_hub:\n\u001b[0;32m--> 464\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minit_git_repo(at_init\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    465\u001b[0m     \u001b[39m# In case of pull, we need to make sure every process has the latest.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m     \u001b[39mif\u001b[39;00m is_torch_tpu_available():\n",
      "File \u001b[0;32m~/langame-worker/env/lib/python3.8/site-packages/transformers/trainer.py:3106\u001b[0m, in \u001b[0;36mTrainer.init_git_repo\u001b[0;34m(self, at_init)\u001b[0m\n\u001b[1;32m   3103\u001b[0m     repo_name \u001b[39m=\u001b[39m get_full_repo_name(repo_name, token\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mhub_token)\n\u001b[1;32m   3105\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3106\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepo \u001b[39m=\u001b[39m Repository(\n\u001b[1;32m   3107\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49moutput_dir,\n\u001b[1;32m   3108\u001b[0m         clone_from\u001b[39m=\u001b[39;49mrepo_name,\n\u001b[1;32m   3109\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   3110\u001b[0m         private\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mhub_private_repo,\n\u001b[1;32m   3111\u001b[0m     )\n\u001b[1;32m   3112\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[1;32m   3113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39moverwrite_output_dir \u001b[39mand\u001b[39;00m at_init:\n\u001b[1;32m   3114\u001b[0m         \u001b[39m# Try again after wiping output_dir\u001b[39;00m\n",
      "File \u001b[0;32m~/langame-worker/env/lib/python3.8/site-packages/huggingface_hub/repository.py:499\u001b[0m, in \u001b[0;36mRepository.__init__\u001b[0;34m(self, local_dir, clone_from, repo_type, use_auth_token, git_user, git_email, revision, private, skip_lfs_files, client)\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhuggingface_token \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[39mif\u001b[39;00m clone_from \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclone_from(repo_url\u001b[39m=\u001b[39;49mclone_from)\n\u001b[1;32m    500\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    501\u001b[0m     \u001b[39mif\u001b[39;00m is_git_repo(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocal_dir):\n",
      "File \u001b[0;32m~/langame-worker/env/lib/python3.8/site-packages/huggingface_hub/repository.py:727\u001b[0m, in \u001b[0;36mRepository.clone_from\u001b[0;34m(self, repo_url, use_auth_token)\u001b[0m\n\u001b[1;32m    724\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(error_msg)\n\u001b[1;32m    726\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m in_repository:\n\u001b[0;32m--> 727\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    728\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mTried to clone a repository in a non-empty folder that isn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt a\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    729\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m git repository. If you really want to do this, do it\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    730\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m manually:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mgit init && git remote add origin && git pull\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    731\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m origin main\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m or clone repo to a new folder and move your\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    732\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m existing files there afterwards.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    733\u001b[0m             )\n\u001b[1;32m    735\u001b[0m \u001b[39mexcept\u001b[39;00m subprocess\u001b[39m.\u001b[39mCalledProcessError \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    736\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(exc\u001b[39m.\u001b[39mstderr)\n",
      "\u001b[0;31mOSError\u001b[0m: Tried to clone a repository in a non-empty folder that isn't a git repository. If you really want to do this, do it manually:\ngit init && git remote add origin && git pull origin main\n or clone repo to a new folder and move your existing files there afterwards."
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    # learning_rate=2e-5,\n",
    "    # weight_decay=0.01,\n",
    "    num_train_epochs=2,\n",
    "    # auto_find_batch_size=True,\n",
    "    # push_to_hub_model_id=\"distilgpt2-starter-classification\",\n",
    "    # push_to_hub_organization=\"Langame\",\n",
    "    # push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"eval\"],\n",
    "    # data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 197.97\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:198 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'If an artificial intelligence were to request permission to exist, what would your decision be? ### artificial intelligence,ai,ai alignment\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "starter = \"If an artificial intelligence were to request permission to exist, what would your decision be? ###\"\n",
    "input_ids = torch.tensor([tokenizer.encode(starter)]).to(\"cuda\")\n",
    "beam_output = model.generate(\n",
    "    input_ids,  \n",
    "    max_length=50, \n",
    "    num_beams=5, \n",
    "    early_stopping=True,\n",
    "    eos_token_id=198,\n",
    ")\n",
    "tokenizer.decode(beam_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in local-pt-checkpoint/tokenizer_config.json\n",
      "Special tokens file saved in local-pt-checkpoint/special_tokens_map.json\n",
      "Configuration saved in local-pt-checkpoint/config.json\n",
      "Model weights saved in local-pt-checkpoint/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"local-pt-checkpoint\")\n",
    "model.save_pretrained(\"local-pt-checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using framework PyTorch: 1.12.0+cu116\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n",
      "/home/gpu/langame-worker/env/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:798: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if batch_size <= 0:\n",
      "Validating ONNX model...\n",
      "\t-[✓] ONNX model output names match reference model ({'logits'})\n",
      "\t- Validating ONNX Model output \"logits\":\n",
      "\t\t-[✓] (2, 8, 50257) matches (2, 8, 50257)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "All good, model saved at: onnx/model.onnx\n"
     ]
    }
   ],
   "source": [
    "!python -m transformers.onnx --model=local-pt-checkpoint onnx/ --feature=causal-lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv onnx/model.onnx distilgpt2-starter-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Tried to clone a repository in a non-empty folder that isn't a git repository. If you really want to do this, do it manually:\ngit init && git remote add origin && git pull origin main\n or clone repo to a new folder and move your existing files there afterwards.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu@91.175.184.114:49512/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb#ch0000023vscode-remote?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mpush_to_hub(\u001b[39m\"\u001b[39;49m\u001b[39mdistilgpt2-starter-classification\u001b[39;49m\u001b[39m\"\u001b[39;49m, private\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, organization\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mLangame\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/langame-worker/env/lib/python3.8/site-packages/transformers/trainer.py:3228\u001b[0m, in \u001b[0;36mTrainer.push_to_hub\u001b[0;34m(self, commit_message, blocking, **kwargs)\u001b[0m\n\u001b[1;32m   3225\u001b[0m \u001b[39m# If a user calls manually `push_to_hub` with `self.args.push_to_hub = False`, we try to create the repo but\u001b[39;00m\n\u001b[1;32m   3226\u001b[0m \u001b[39m# it might fail.\u001b[39;00m\n\u001b[1;32m   3227\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrepo\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 3228\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minit_git_repo()\n\u001b[1;32m   3230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mshould_save:\n\u001b[1;32m   3231\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mhub_model_id \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/langame-worker/env/lib/python3.8/site-packages/transformers/trainer.py:3106\u001b[0m, in \u001b[0;36mTrainer.init_git_repo\u001b[0;34m(self, at_init)\u001b[0m\n\u001b[1;32m   3103\u001b[0m     repo_name \u001b[39m=\u001b[39m get_full_repo_name(repo_name, token\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mhub_token)\n\u001b[1;32m   3105\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3106\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepo \u001b[39m=\u001b[39m Repository(\n\u001b[1;32m   3107\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49moutput_dir,\n\u001b[1;32m   3108\u001b[0m         clone_from\u001b[39m=\u001b[39;49mrepo_name,\n\u001b[1;32m   3109\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   3110\u001b[0m         private\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mhub_private_repo,\n\u001b[1;32m   3111\u001b[0m     )\n\u001b[1;32m   3112\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[1;32m   3113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39moverwrite_output_dir \u001b[39mand\u001b[39;00m at_init:\n\u001b[1;32m   3114\u001b[0m         \u001b[39m# Try again after wiping output_dir\u001b[39;00m\n",
      "File \u001b[0;32m~/langame-worker/env/lib/python3.8/site-packages/huggingface_hub/repository.py:499\u001b[0m, in \u001b[0;36mRepository.__init__\u001b[0;34m(self, local_dir, clone_from, repo_type, use_auth_token, git_user, git_email, revision, private, skip_lfs_files, client)\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhuggingface_token \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[39mif\u001b[39;00m clone_from \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclone_from(repo_url\u001b[39m=\u001b[39;49mclone_from)\n\u001b[1;32m    500\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    501\u001b[0m     \u001b[39mif\u001b[39;00m is_git_repo(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocal_dir):\n",
      "File \u001b[0;32m~/langame-worker/env/lib/python3.8/site-packages/huggingface_hub/repository.py:727\u001b[0m, in \u001b[0;36mRepository.clone_from\u001b[0;34m(self, repo_url, use_auth_token)\u001b[0m\n\u001b[1;32m    724\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(error_msg)\n\u001b[1;32m    726\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m in_repository:\n\u001b[0;32m--> 727\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    728\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mTried to clone a repository in a non-empty folder that isn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt a\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    729\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m git repository. If you really want to do this, do it\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    730\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m manually:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mgit init && git remote add origin && git pull\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    731\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m origin main\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m or clone repo to a new folder and move your\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    732\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m existing files there afterwards.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    733\u001b[0m             )\n\u001b[1;32m    735\u001b[0m \u001b[39mexcept\u001b[39;00m subprocess\u001b[39m.\u001b[39mCalledProcessError \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    736\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(exc\u001b[39m.\u001b[39mstderr)\n",
      "\u001b[0;31mOSError\u001b[0m: Tried to clone a repository in a non-empty folder that isn't a git repository. If you really want to do this, do it manually:\ngit init && git remote add origin && git pull origin main\n or clone repo to a new folder and move your existing files there afterwards."
     ]
    }
   ],
   "source": [
    "trainer.push_to_hub(\"distilgpt2-starter-classification\", private=True, organization=\"Langame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:198 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['artificial intelligence', 'ai alignment', 'ai']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Langame/distilgpt2-starter-classification\", use_auth_token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Langame/distilgpt2-starter-classification\", use_auth_token=True)\n",
    "tokenizer.eos_token_id = 198\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "device = \"cuda:0\" if True else \"cpu\"\n",
    "if device == \"cuda:0\":\n",
    "    model.cuda()\n",
    "import torch\n",
    "starter = \"If an artificial intelligence were to request permission to exist, what would your decision be? ###\"\n",
    "input_ids = torch.tensor([tokenizer.encode(starter)]).to(\"cuda\")\n",
    "beam_output = model.generate(\n",
    "    input_ids,  \n",
    "    return_dict_in_generate=True,\n",
    "    eos_token_id=198,  # line break\n",
    "    max_length=50,\n",
    "    num_return_sequences=1,\n",
    "    return_text=False,\n",
    "    return_full_text=False,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "topics = tokenizer.decode(beam_output[\"sequences\"].tolist()[0]).replace(\n",
    "    starter, \"\"\n",
    ").strip()  # TODO: return_text doesn't work for some reason\n",
    "list(set(topics.split(\",\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': ' history,ai,artificial intelligence\\n'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "API_KEY = \"YOUR_API_KEY\"\n",
    "API_URL = \"https://api-inference.huggingface.co/models/Langame/distilgpt2-starter-classification\"\n",
    "headers = {\"Authorization\": f\"Bearer {API_KEY}\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\n",
    "output = query({\n",
    "\t\"inputs\": starter,\n",
    "\t\"parameters\": {\n",
    "\t\t\"max_length\": 50,\n",
    "\t\t\"num_return_sequences\": 1,\n",
    "\t\t\"return_text\": False,\n",
    "\t\t\"return_full_text\": False,\n",
    "\t\t\"do_sample\": True,\n",
    "\t\t\"top_k\": 50,\n",
    "\t\t\"top_p\": 0.95,\n",
    "\t\t\"end_sequence\": \"\\n\",\n",
    "\t},\n",
    "\t\"options\": {\n",
    "\t\t\"wait_for_model\": True,\n",
    "\t\t\"use_cache\": True,  # TODO: should be in public api args\n",
    "\t},\n",
    "})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoCredentialsError",
     "evalue": "Unable to locate credentials",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoCredentialsError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hq/swhzs3js29z7mb5b7ycnjcnm0000gn/T/ipykernel_5375/558471544.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrole\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_execution_role\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/langame-worker/env/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mget_execution_role\u001b[0;34m(sagemaker_session)\u001b[0m\n\u001b[1;32m   4662\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msagemaker_session\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4663\u001b[0m         \u001b[0msagemaker_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4664\u001b[0;31m     \u001b[0marn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_caller_identity_arn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\":role/\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/langame-worker/env/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mget_caller_identity_arn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3693\u001b[0m                 )\n\u001b[1;32m   3694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3695\u001b[0;31m         assumed_role = self.boto_session.client(\n\u001b[0m\u001b[1;32m   3696\u001b[0m             \u001b[0;34m\"sts\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3697\u001b[0m             \u001b[0mregion_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboto_region_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/langame-worker/env/lib/python3.8/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m                 )\n\u001b[1;32m    507\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/langame-worker/env/lib/python3.8/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0mapply_request_checksum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m             http, parsed_response = self._make_request(\n\u001b[0m\u001b[1;32m    899\u001b[0m                 \u001b[0moperation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             )\n",
      "\u001b[0;32m~/Documents/langame-worker/env/lib/python3.8/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, operation_model, request_dict, request_context)\u001b[0m\n\u001b[1;32m    919\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_endpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m             self.meta.events.emit(\n",
      "\u001b[0;32m~/Documents/langame-worker/env/lib/python3.8/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36mmake_request\u001b[0;34m(self, operation_model, request_dict)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         )\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/langame-worker/env/lib/python3.8/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, request_dict, operation_model)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'context'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_retries_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattempts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         success_response, exception = self._get_response(\n\u001b[1;32m    200\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/langame-worker/env/lib/python3.8/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36mcreate_request\u001b[0;34m(self, params, operation_model)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0mservice_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mservice_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moperation_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             )\n\u001b[0;32m--> 134\u001b[0;31m             self._event_emitter.emit(\n\u001b[0m\u001b[1;32m    135\u001b[0m                 \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/langame-worker/env/lib/python3.8/site-packages/botocore/hooks.py\u001b[0m in \u001b[0;36memit\u001b[0;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0memit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0maliased_event_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_event_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_emitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maliased_event_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0memit_until_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/langame-worker/env/lib/python3.8/site-packages/botocore/hooks.py\u001b[0m in \u001b[0;36memit\u001b[0;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m                  \u001b[0mhandlers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \"\"\"\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_emit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0memit_until_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/langame-worker/env/lib/python3.8/site-packages/botocore/hooks.py\u001b[0m in \u001b[0;36m_emit\u001b[0;34m(self, event_name, kwargs, stop_on_response)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers_to_call\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Event %s: calling handler %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m             \u001b[0mresponses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop_on_response\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/langame-worker/env/lib/python3.8/site-packages/botocore/signers.py\u001b[0m in \u001b[0;36mhandler\u001b[0;34m(self, operation_name, request, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# this method is invoked to sign the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;31m# Don't call this method directly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     def sign(\n",
      "\u001b[0;32m~/Documents/langame-worker/env/lib/python3.8/site-packages/botocore/signers.py\u001b[0m in \u001b[0;36msign\u001b[0;34m(self, operation_name, request, region_name, signing_type, expires_in, signing_name)\u001b[0m\n\u001b[1;32m    185\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_choose_signer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigning_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/langame-worker/env/lib/python3.8/site-packages/botocore/auth.py\u001b[0m in \u001b[0;36madd_auth\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredentials\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNoCredentialsError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0mdatetime_now\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutcnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime_now\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSIGV4_TIMESTAMP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoCredentialsError\u001b[0m: Unable to locate credentials"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must setup local AWS configuration with a region supported by SageMaker.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hq/swhzs3js29z7mb5b7ycnjcnm0000gn/T/ipykernel_4791/1971253684.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# gets role for executing training job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mrole\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_execution_role\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m hyperparameters = {\n\u001b[1;32m      7\u001b[0m         \u001b[0;34m'model_name_or_path'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'gpt2-medium'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/langame-worker/env/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mget_execution_role\u001b[0;34m(sagemaker_session)\u001b[0m\n\u001b[1;32m   4661\u001b[0m     \"\"\"\n\u001b[1;32m   4662\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msagemaker_session\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4663\u001b[0;31m         \u001b[0msagemaker_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4664\u001b[0m     \u001b[0marn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_caller_identity_arn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/langame-worker/env/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, boto_session, sagemaker_client, sagemaker_runtime_client, sagemaker_featurestore_runtime_client, default_bucket, settings)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         self._initialize(\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0mboto_session\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mboto_session\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0msagemaker_client\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/langame-worker/env/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, boto_session, sagemaker_client, sagemaker_runtime_client, sagemaker_featurestore_runtime_client)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_region_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboto_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregion_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_region_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    151\u001b[0m                 \u001b[0;34m\"Must setup local AWS configuration with a region supported by SageMaker.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: Must setup local AWS configuration with a region supported by SageMaker."
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# gets role for executing training job\n",
    "role = sagemaker.get_execution_role()\n",
    "hyperparameters = {\n",
    "\t'model_name_or_path':'gpt2-medium',\n",
    "\t'output_dir':'./model'\n",
    "\t# add your remaining hyperparameters\n",
    "\t# more info here https://github.com/huggingface/transformers/tree/v4.17.0/examples/pytorch/language-modeling\n",
    "}\n",
    "\n",
    "# git configuration to download our fine-tuning script\n",
    "git_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.17.0'}\n",
    "\n",
    "# creates Hugging Face estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "\tentry_point='run_clm.py',\n",
    "\tsource_dir='./examples/pytorch/language-modeling',\n",
    "\tinstance_type='ml.p3.2xlarge',\n",
    "\tinstance_count=1,\n",
    "\trole=role,\n",
    "\tgit_config=git_config,\n",
    "\ttransformers_version='4.17.0',\n",
    "\tpytorch_version='1.10.2',\n",
    "\tpy_version='py38',\n",
    "\thyperparameters = hyperparameters\n",
    ")\n",
    "\n",
    "# starting the train job\n",
    "huggingface_estimator.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b3c97174413c2100d2f2441a272ac4a9b6aae507e3bd1b85b4c1c7cd94685bf3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
