{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langame import LangameClient\n",
    "import json\n",
    "import openai\n",
    "import datetime\n",
    "from langame.conversation_starters import get_existing_conversation_starters\n",
    "import logging\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer, AutoTokenizer, DataCollatorForLanguageModeling, DataCollatorForLanguageModeling\n",
    "\n",
    "c = LangameClient(\"../config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# topic classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file_name = f\"../data/fine_tune_topic_classification_{datetime.date.today().strftime('%d_%m_%Y')}.jsonl\"\n",
    "\n",
    "with open(out_file_name, 'w') as outfile:\n",
    "    for e in c._firestore_client.collection(\"memes\").stream():\n",
    "        json.dump({\n",
    "            \"text\": e.get(\"content\"),\n",
    "            \"label\": e.get(\"topics\")[0]\n",
    "        }, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_f = openai.File.create(\n",
    "    file=open(file_name),\n",
    "    purpose=\"classifications\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.Classification.create(\n",
    "    file=\"file-rV9rZbiiziXLfjqc4MKLLChy\",\n",
    "    query=\"123\",\n",
    "    search_model=\"ada\", \n",
    "    model=\"curie\", \n",
    "    max_examples=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# quality classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping id: 4XlXSjPhH6QSGZDzwIQe, garbage\n",
      "Skipping id: CYsOWtp1ed2XDeplB5O5, garbage\n",
      "Skipping id: DlXglThZUiPJcuyLLKHV, garbage\n",
      "Skipping id: NtXdp7ODn4sR3YR2eEW6, garbage\n",
      "Skipping id: U0DUXOSpfqYsgurOyL3L, garbage\n",
      "Skipping id: Uikgs6eO85MRivL0peL6, garbage\n",
      "Skipping id: XIPyRwJloK6ZdAdTHOeu, garbage\n",
      "Skipping id: d9FxG5haHnN2ku7spAng, garbage\n",
      "Skipping id: eyzSmD1AdfKkwOYtfoof, garbage\n",
      "Skipping id: f1ojlkGYgnOI4MLRGC4T, garbage\n",
      "Skipping id: fySI8xbjfC5hx7QdHpva, garbage\n",
      "Skipping id: gLDxkmHFL3OFufeje6E8, garbage\n",
      "Skipping id: jkkOtT3PVoKQMmpA947Y, garbage\n",
      "Skipping id: l1HjGNlNwpFzTnCsJxJA, garbage\n",
      "Skipping id: o7jTZJyRoBxaDxIQwyQg, garbage\n",
      "Skipping id: osfrRcogOlIrEtxp5fpI, garbage\n",
      "Skipping id: pXXGv1o80FIldPiytsN7, garbage\n",
      "Skipping id: pf1tiPMceCwvXLUfKQwY, garbage\n",
      "Skipping id: q2CEZCITA0RVSsfTQRP0, garbage\n",
      "Skipping id: qAwPXAX5efpoQt0TLoUu, garbage\n",
      "Skipping id: qXEJ0wGr86cw8CXIgL4z, garbage\n",
      "Skipping id: r5vxgYs9FnZYOxrh2tcj, garbage\n",
      "Skipping id: rdguN3u8mWRuiRXfUKM8, garbage\n",
      "Skipping id: svY8fharA04Fga7FAki5, garbage\n",
      "Skipping id: svZxcinrbiybvsbqk3fT, garbage\n",
      "Skipping id: t58QkfO5GsncvNSmtpQi, garbage\n",
      "Skipping id: uTDyItbdhyWZoKV3Kc23, garbage\n",
      "Skipping id: vDiOKT9fzERg9sasNjMA, garbage\n",
      "Skipping id: vPifbxb1aFBty4kTmrAF, garbage\n",
      "Skipping id: vp4TdnJzy1HbfgTBMIGU, garbage\n",
      "Skipping id: wHotBw9c4TWfO840fYIL, garbage\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8 omp threads (processes), consider increasing --nb_cores if you have more\n",
      "Launching the whole pipeline 05/01/2022, 10:25:03\n",
      "There are 2052 embeddings of dim 768\n",
      "\tCompute estimated construction time of the index 05/01/2022, 10:25:03\n",
      "\t\t-> Train: 16.7 minutes\n",
      "\t\t-> Add: 0.0 seconds\n",
      "\t\tTotal: 16.7 minutes\n",
      "\t>>> Finished \"Compute estimated construction time of the index\" in 0.0001 secs\n",
      "\tChecking that your have enough memory available to create the index 05/01/2022, 10:25:03\n",
      "7.5MB of memory will be needed to build the index (more might be used if you have more)\n",
      "\t>>> Finished \"Checking that your have enough memory available to create the index\" in 0.0004 secs\n",
      "\tSelecting most promising index types given data characteristics 05/01/2022, 10:25:03\n",
      "\t>>> Finished \"Selecting most promising index types given data characteristics\" in 0.0000 secs\n",
      "\tCreating the index 05/01/2022, 10:25:03\n",
      "\t\t-> Instanciate the index HNSW15 05/01/2022, 10:25:03\n",
      "\t\t>>> Finished \"-> Instanciate the index HNSW15\" in 0.0034 secs\n",
      "The index size will be approximately 6.2MB\n",
      "The memory available for adding the vectors is 7.0GB(total available - used by the index)\n",
      "Will be using at most 1GB of ram for adding\n",
      "\t\t-> Adding the vectors to the index 05/01/2022, 10:25:03\n",
      "Using a batch size of 325520 (memory overhead 953.7MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 273.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t>>> Finished \"-> Adding the vectors to the index\" in 0.0805 secs\n",
      "\t>>> Finished \"Creating the index\" in 0.0856 secs\n",
      "\tComputing best hyperparameters 05/01/2022, 10:25:03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>>> Finished \"Computing best hyperparameters\" in 2.8843 secs\n",
      "The best hyperparameters are: efSearch=16383\n",
      "\tCompute fast metrics 05/01/2022, 10:25:06\n",
      "2000\n",
      "\t>>> Finished \"Compute fast metrics\" in 6.9442 secs\n",
      "\tSaving the index on local disk 05/01/2022, 10:25:13\n",
      "\t>>> Finished \"Saving the index on local disk\" in 0.0144 secs\n",
      "Recap:\n",
      "{'99p_search_speed_ms': 4.525868840003113,\n",
      " 'avg_search_speed_ms': 3.4529195985000882,\n",
      " 'compression ratio': 0.9576115129509153,\n",
      " 'index_key': 'HNSW15',\n",
      " 'index_param': 'efSearch=16383',\n",
      " 'nb vectors': 2052,\n",
      " 'reconstruction error %': 0.0,\n",
      " 'size in bytes': 6582778,\n",
      " 'vectors dimension': 768}\n",
      ">>> Finished \"Launching the whole pipeline\" in 10.2528 secs\n"
     ]
    }
   ],
   "source": [
    "from langame.conversation_starters import get_existing_conversation_starters\n",
    "import logging\n",
    "logger = logging.getLogger(\"classification\")\n",
    "memes, index, embeddings_model = get_existing_conversation_starters(\n",
    "    c._firestore_client, logger=logger, confirmed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file_name = f\"../data/fine_tune_fitness_classification_{datetime.date.today().strftime('%d_%m_%Y')}.jsonl\"\n",
    "\n",
    "for e in memes:\n",
    "    with open(out_file_name, \"a+\") as outfile:\n",
    "        json.dump({\n",
    "            \"prompt\": f\"{','.join(e['topics'])} ### {e['content']} ~~~\",\n",
    "            \"completion\": f\" 1\\n\",\n",
    "        }, outfile)\n",
    "        outfile.write('\\n')\n",
    "for e in c._firestore_client.collection(\"deleted_memes\").stream():\n",
    "    # check has \"topics\" and \"content\"\n",
    "    if \"topics\" not in e.to_dict() or \"content\" not in e.to_dict():\n",
    "        continue\n",
    "    with open(out_file_name, \"a+\") as outfile:\n",
    "        json.dump({\n",
    "            \"prompt\": f\"{','.join(e.get('topics'))} ### {e.get('content')} ~~~\",\n",
    "            \"completion\": f\" 0\\n\",\n",
    "        }, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[34;1m\"prompt\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"ice breaker ### When is a time when you know for sure you'll soon have to exchange a nice, meaningful conversation with someone new? ~~~\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"completion\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\" 1\\n\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n",
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[34;1m\"prompt\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"space exploration,space travel ### Do you think humans are the only intelligent life in the universe? ~~~\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"completion\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\" 1\\n\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n",
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[34;1m\"prompt\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"ecology ### Have natural disasters gotten worse with the increase in human existence? If so, why? ~~~\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"completion\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\" 1\\n\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n",
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[34;1m\"prompt\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"ice breaker ### What would prompt you to make a dramatic change in your life. Is it worth it? ~~~\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"completion\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\" 1\\n\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n",
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[34;1m\"prompt\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"philosophy,mathematic ### Why bother with subjectivity when there are open ended conjectures? ~~~\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"completion\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\" 1\\n\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!head -5 $out_file_name | jq ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "execute: openai tools fine_tunes.prepare_data -f /Users/louisbeaumont/Documents/langame-worker/notebooks/../data/fine_tune_fitness_classification_01_05_2022.jsonl\n"
     ]
    }
   ],
   "source": [
    "!echo \"execute: openai tools fine_tunes.prepare_data -f $(pwd)/$out_file_name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FineTune fine-tune id=ft-bBksgsOcWJjeZuV9XHiHSvIs at 0x158877a90> JSON: {\n",
       "  \"created_at\": 1651376064,\n",
       "  \"events\": [\n",
       "    {\n",
       "      \"created_at\": 1651376064,\n",
       "      \"level\": \"info\",\n",
       "      \"message\": \"Created fine-tune: ft-bBksgsOcWJjeZuV9XHiHSvIs\",\n",
       "      \"object\": \"fine-tune-event\"\n",
       "    }\n",
       "  ],\n",
       "  \"fine_tuned_model\": null,\n",
       "  \"hyperparams\": {\n",
       "    \"batch_size\": null,\n",
       "    \"learning_rate_multiplier\": null,\n",
       "    \"n_epochs\": 4,\n",
       "    \"prompt_loss_weight\": 0.1\n",
       "  },\n",
       "  \"id\": \"ft-bBksgsOcWJjeZuV9XHiHSvIs\",\n",
       "  \"model\": \"ada\",\n",
       "  \"object\": \"fine-tune\",\n",
       "  \"organization_id\": \"org-KwcHNgfGe4pqdKDLQIJt99UZ\",\n",
       "  \"result_files\": [],\n",
       "  \"status\": \"pending\",\n",
       "  \"training_files\": [\n",
       "    {\n",
       "      \"bytes\": 586199,\n",
       "      \"created_at\": 1651376061,\n",
       "      \"filename\": \"_prepared_train.jsonl\",\n",
       "      \"id\": \"file-FjpwIA5Q2FH7afulYnA5Z27f\",\n",
       "      \"object\": \"file\",\n",
       "      \"purpose\": \"fine-tune\",\n",
       "      \"status\": \"uploaded\",\n",
       "      \"status_details\": null\n",
       "    }\n",
       "  ],\n",
       "  \"updated_at\": 1651376064,\n",
       "  \"validation_files\": [\n",
       "    {\n",
       "      \"bytes\": 148790,\n",
       "      \"created_at\": 1651376063,\n",
       "      \"filename\": \"_prepared_valid.jsonl\",\n",
       "      \"id\": \"file-leccUbyFFt0VYaOxiBIpVqyf\",\n",
       "      \"object\": \"file\",\n",
       "      \"purpose\": \"fine-tune\",\n",
       "      \"status\": \"uploaded\",\n",
       "      \"status_details\": null\n",
       "    }\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file = openai.File.create(\n",
    "  file=open(\"_prepared_train.jsonl\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "valid_file = openai.File.create(\n",
    "  file=open(\"_prepared_valid.jsonl\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "openai.FineTune.create(\n",
    "    training_file=train_file[\"id\"],\n",
    "    validation_file=valid_file[\"id\"],\n",
    "    model=\"ada\",\n",
    "    # \"On classification tasks, we recommend setting this to false\"\n",
    "    use_packing=False,\n",
    "    # https://beta.openai.com/docs/api-reference/fine-tunes/create#fine-tunes/create-prompt_loss_weight\n",
    "    # classification recommended 0.1 (it's the default value anyway)\n",
    "    prompt_loss_weight=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf _prepared_train.jsonl _prepared_valid.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom model topic inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping id: 4XlXSjPhH6QSGZDzwIQe, garbage\n",
      "Skipping id: 4sIjdMJr0exliT9yfvuL, garbage\n",
      "Skipping id: 6jNOw0e7VA6lHWddtqMy, garbage\n",
      "Skipping id: 9ZfEEkrB3cjLooW6YZM2, garbage\n",
      "Skipping id: B8hRGbGGOweqEyxuuplE, garbage\n",
      "Skipping id: CYsOWtp1ed2XDeplB5O5, garbage\n",
      "Skipping id: DlXglThZUiPJcuyLLKHV, garbage\n",
      "Skipping id: EBZCFk0aZPVS1SinJwcz, garbage\n",
      "Skipping id: FP0UBU9PBzttN2d7YFvZ, garbage\n",
      "Skipping id: ISOBHbvYDxi04VEFPZWw, garbage\n",
      "Skipping id: Iu1De6gLJ0z7eoi7Qb96, garbage\n",
      "Skipping id: NtXdp7ODn4sR3YR2eEW6, garbage\n",
      "Skipping id: P05T4keIh8bvF1SKVSSw, garbage\n",
      "Skipping id: RCI4BcA6Bf1kYBk7hUTe, garbage\n",
      "Skipping id: U0DUXOSpfqYsgurOyL3L, garbage\n",
      "Skipping id: Uikgs6eO85MRivL0peL6, garbage\n",
      "Skipping id: XIPyRwJloK6ZdAdTHOeu, garbage\n",
      "Skipping id: XdaHW0tJ9wIGAos7WoG4, garbage\n",
      "Skipping id: Y2V1ZLMVggTj45u8RmzK, garbage\n",
      "Skipping id: bCW1uPMxDEy6iO3oMRQq, garbage\n",
      "Skipping id: bdf0W2iG9B9rZXbe4xun, garbage\n",
      "Skipping id: d9FxG5haHnN2ku7spAng, garbage\n",
      "Skipping id: eyzSmD1AdfKkwOYtfoof, garbage\n",
      "Skipping id: f1ojlkGYgnOI4MLRGC4T, garbage\n",
      "Skipping id: fnprJBuBWAE9HH2LqHV6, garbage\n",
      "Skipping id: fySI8xbjfC5hx7QdHpva, garbage\n",
      "Skipping id: gLDxkmHFL3OFufeje6E8, garbage\n",
      "Skipping id: gUXUeStEjNZwDE1j5l8l, garbage\n",
      "Skipping id: gYsv5IG0NS1vkgdmgL2O, garbage\n",
      "Skipping id: hDybtBa855kDWy9l655v, garbage\n",
      "Skipping id: hoguvPcBPEnK4QbuXIp1, garbage\n",
      "Skipping id: i58xevUa8NEItnlX0Jzk, garbage\n",
      "Skipping id: idy5E0oY8I1HevGY7Dlk, garbage\n",
      "Skipping id: jkkOtT3PVoKQMmpA947Y, garbage\n",
      "Skipping id: kI7uqiRAJkdBP7QTaTZX, garbage\n",
      "Skipping id: l1HjGNlNwpFzTnCsJxJA, garbage\n",
      "Skipping id: lzjy6lscTmTj1gcdIkNl, garbage\n",
      "Skipping id: o7jTZJyRoBxaDxIQwyQg, garbage\n",
      "Skipping id: osfrRcogOlIrEtxp5fpI, garbage\n",
      "Skipping id: pXXGv1o80FIldPiytsN7, garbage\n",
      "Skipping id: pf1tiPMceCwvXLUfKQwY, garbage\n",
      "Skipping id: q2CEZCITA0RVSsfTQRP0, garbage\n",
      "Skipping id: qAwPXAX5efpoQt0TLoUu, garbage\n",
      "Skipping id: qXEJ0wGr86cw8CXIgL4z, garbage\n",
      "Skipping id: r5vxgYs9FnZYOxrh2tcj, garbage\n",
      "Skipping id: rdguN3u8mWRuiRXfUKM8, garbage\n",
      "Skipping id: svY8fharA04Fga7FAki5, garbage\n",
      "Skipping id: svZxcinrbiybvsbqk3fT, garbage\n",
      "Skipping id: t58QkfO5GsncvNSmtpQi, garbage\n",
      "Skipping id: uTDyItbdhyWZoKV3Kc23, garbage\n",
      "Skipping id: uY5HtuacEZGzUXY4D4Lm, garbage\n",
      "Skipping id: ukpHKukvcNPlkhPaTjb5, garbage\n",
      "Skipping id: vDiOKT9fzERg9sasNjMA, garbage\n",
      "Skipping id: vPifbxb1aFBty4kTmrAF, garbage\n",
      "Skipping id: vp4TdnJzy1HbfgTBMIGU, garbage\n",
      "Skipping id: wHotBw9c4TWfO840fYIL, garbage\n",
      "Skipping id: wlKKB1Mkm7GZpnEQjp9w, garbage\n",
      "Skipping id: xfXFHDkShv17nTMG2rk2, garbage\n",
      "Skipping id: yILwvdGuUBtf5h0U1FWM, garbage\n",
      "Skipping id: yYiXnfz1JA3zrikGBffE, garbage\n",
      "Skipping id: zm29pjekcPNeTWbaQn1U, garbage\n"
     ]
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "Column 1 named validation expected length 1956 but got length 490",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu@91.175.184.114:49512/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb#ch0000013vscode-remote?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlogging\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu@91.175.184.114:49512/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb#ch0000013vscode-remote?line=2'>3</a>\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m\"\u001b[39m\u001b[39mclassification\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu@91.175.184.114:49512/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb#ch0000013vscode-remote?line=3'>4</a>\u001b[0m memes, index, embeddings_model \u001b[39m=\u001b[39m get_existing_conversation_starters(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu@91.175.184.114:49512/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb#ch0000013vscode-remote?line=4'>5</a>\u001b[0m     c\u001b[39m.\u001b[39;49m_firestore_client, logger\u001b[39m=\u001b[39;49mlogger, confirmed\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, push_to_hub\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/langame-worker/langame/conversation_starters.py:70\u001b[0m, in \u001b[0;36mget_existing_conversation_starters\u001b[0;34m(client, logger, use_gpu, limit, batch_embeddings_size, confirmed, push_to_hub)\u001b[0m\n\u001b[1;32m     65\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m     66\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(existing_conversation_starters)\u001b[39m}\u001b[39;00m\u001b[39m existing conversation starters\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m     )\n\u001b[1;32m     68\u001b[0m \u001b[39mif\u001b[39;00m push_to_hub:\n\u001b[1;32m     69\u001b[0m     \u001b[39m# TODO: clean dataset no id...\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     dataset \u001b[39m=\u001b[39m Dataset\u001b[39m.\u001b[39;49mfrom_dict({\n\u001b[1;32m     71\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m: existing_conversation_starters[:\u001b[39mint\u001b[39;49m(\u001b[39mlen\u001b[39;49m(existing_conversation_starters) \u001b[39m*\u001b[39;49m \u001b[39m0.8\u001b[39;49m)],\n\u001b[1;32m     72\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mvalidation\u001b[39;49m\u001b[39m\"\u001b[39;49m: existing_conversation_starters[\u001b[39mint\u001b[39;49m(\u001b[39mlen\u001b[39;49m(existing_conversation_starters) \u001b[39m*\u001b[39;49m \u001b[39m0.8\u001b[39;49m):],\n\u001b[1;32m     73\u001b[0m     })\n\u001b[1;32m     74\u001b[0m     dataset\u001b[39m.\u001b[39mpush_to_hub(\u001b[39m\"\u001b[39m\u001b[39mLangame/starter\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     76\u001b[0m \u001b[39mif\u001b[39;00m logger:\n",
      "File \u001b[0;32m~/langame-worker/env/lib/python3.8/site-packages/datasets/arrow_dataset.py:859\u001b[0m, in \u001b[0;36mDataset.from_dict\u001b[0;34m(cls, mapping, features, info, split)\u001b[0m\n\u001b[1;32m    854\u001b[0m     mapping \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mencode_batch(mapping)\n\u001b[1;32m    855\u001b[0m mapping \u001b[39m=\u001b[39m {\n\u001b[1;32m    856\u001b[0m     col: OptimizedTypedSequence(data, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39mfeatures[col] \u001b[39mif\u001b[39;00m features \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, col\u001b[39m=\u001b[39mcol)\n\u001b[1;32m    857\u001b[0m     \u001b[39mfor\u001b[39;00m col, data \u001b[39min\u001b[39;00m mapping\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    858\u001b[0m }\n\u001b[0;32m--> 859\u001b[0m pa_table \u001b[39m=\u001b[39m InMemoryTable\u001b[39m.\u001b[39;49mfrom_pydict(mapping\u001b[39m=\u001b[39;49mmapping)\n\u001b[1;32m    860\u001b[0m \u001b[39mif\u001b[39;00m info\u001b[39m.\u001b[39mfeatures \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    861\u001b[0m     info\u001b[39m.\u001b[39mfeatures \u001b[39m=\u001b[39m Features({col: ts\u001b[39m.\u001b[39mget_inferred_type() \u001b[39mfor\u001b[39;00m col, ts \u001b[39min\u001b[39;00m mapping\u001b[39m.\u001b[39mitems()})\n",
      "File \u001b[0;32m~/langame-worker/env/lib/python3.8/site-packages/datasets/table.py:750\u001b[0m, in \u001b[0;36mInMemoryTable.from_pydict\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pydict\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    736\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \u001b[39m    Construct a Table from Arrow arrays or columns\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[39m        :class:`datasets.table.Table`:\u001b[39;00m\n\u001b[1;32m    749\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(pa\u001b[39m.\u001b[39;49mTable\u001b[39m.\u001b[39;49mfrom_pydict(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[0;32m~/langame-worker/env/lib/python3.8/site-packages/pyarrow/table.pxi:3625\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_pydict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/langame-worker/env/lib/python3.8/site-packages/pyarrow/table.pxi:5151\u001b[0m, in \u001b[0;36mpyarrow.lib._from_pydict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/langame-worker/env/lib/python3.8/site-packages/pyarrow/table.pxi:3574\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_arrays\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/langame-worker/env/lib/python3.8/site-packages/pyarrow/table.pxi:2793\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.validate\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/langame-worker/env/lib/python3.8/site-packages/pyarrow/error.pxi:100\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: Column 1 named validation expected length 1956 but got length 490"
     ]
    }
   ],
   "source": [
    "\n",
    "logger = logging.getLogger(\"classification\")\n",
    "memes, index, embeddings_model = get_existing_conversation_starters(\n",
    "    c._firestore_client, logger=logger, confirmed=True, push_to_hub=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out_file_name = f\"../data/fine_tune_topic_classification_{datetime.date.today().strftime('%d_%m_%Y')}.txt\"\n",
    "\n",
    "# make a dict of {\"TOPIC\": int}\n",
    "topic_to_int = {}\n",
    "for e in memes:\n",
    "    for topic in e['topics']:\n",
    "        if topic not in topic_to_int:\n",
    "            topic_to_int[topic] = len(topic_to_int)\n",
    "try:\n",
    "    os.remove(out_file_name)\n",
    "except:\n",
    "    pass\n",
    "for e in memes:\n",
    "    with open(out_file_name, \"a+\") as outfile:\n",
    "        if len(e['topics']) == 0: continue\n",
    "        outfile.write(f\"{e['content']} ### {','.join(map(str, e['topics']))}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "When is a time when you know for sure you'll soon have to exchange a nice, meaningful conversation with someone new? ### ice breaker\n",
      "Do you think humans are the only intelligent life in the universe? ### space exploration,space travel\n",
      "Have natural disasters gotten worse with the increase in human existence? If so, why? ### ecology\n"
     ]
    }
   ],
   "source": [
    "!head $out_file_name -n3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-f18bc8eb4d57408a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /home/gpu/.cache/huggingface/datasets/text/default-f18bc8eb4d57408a/0.0.0/acc32f2f2ef863c93c2f30c52f7df6cc9053a1c2230b8d7da0d210404683ca08...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abea66a96a9348018341d47d014db85b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23866c0d97f4db28c9edcde9b66fa56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34558ed324c46c0ba8c4c8ff8f1d043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e63002f27640caa02808f37a3c71cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /home/gpu/.cache/huggingface/datasets/text/default-f18bc8eb4d57408a/0.0.0/acc32f2f2ef863c93c2f30c52f7df6cc9053a1c2230b8d7da0d210404683ca08. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c765a09d7394222bdaf643ac589a8fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# split the file into train.txt and eval.txt\n",
    "with open(out_file_name, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    train_lines = lines[:int(len(lines) * 0.8)]\n",
    "    eval_lines = lines[int(len(lines) * 0.8):]\n",
    "    with open(\"train.txt\", \"w\") as f:\n",
    "        f.writelines(train_lines)\n",
    "    with open(\"eval.txt\", \"w\") as f:\n",
    "        f.writelines(eval_lines)\n",
    "dataset = load_dataset('text', data_files={\"train\": \"train.txt\", \"eval\": \"eval.txt\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /home/gpu/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"distilgpt2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 6,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilgpt2/resolve/main/pytorch_model.bin from cache at /home/gpu/.cache/huggingface/transformers/43a212e83e76bcb07f45be584cf100676bdbbbe9c13f9e5c1c050049143a832f.a83d881ec4d624fd4b5826dd026e315246c48c67504ff91c0500570e291a54ba\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /home/gpu/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"distilgpt2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 6,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilgpt2/resolve/main/vocab.json from cache at /home/gpu/.cache/huggingface/transformers/55051ac97dcc32f0a736d21a32a4d42b0d9b90f117ca7c38e65038b04bd5c3f5.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "loading file https://huggingface.co/distilgpt2/resolve/main/merges.txt from cache at /home/gpu/.cache/huggingface/transformers/9dfb299b74cdf7601ba7cd3a8073dbdac351caec0ed7ab5849b098b3c8ae3d57.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/distilgpt2/resolve/main/tokenizer.json from cache at /home/gpu/.cache/huggingface/transformers/accb287b5a5396b2597382916b6cc939fdab1366e89475a92338d3971b3d02b7.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "loading file https://huggingface.co/distilgpt2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilgpt2/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilgpt2/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /home/gpu/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"distilgpt2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 6,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c2279135d24003af22082f628aa6c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c89994fc1f654404865cb974496f6637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dea3f0254ff40f79a6b84486de746f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dba50a5b73548f79e7e06ffafddc355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121e631c9e83415f9c433641aaf2f509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b123d751b5a14c9b8319e6131368b73e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d113ef2f505843d2a28bcb72aed56641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4690a6a9c46845e293bff3aafd3eaca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd070cd3f90049378bf94b396a5abe63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69132126a9874917b684b18dddad976f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b52933da44f4bd7a2f90902251d7bc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6964211bf73b4cf1b33271d90eb3b82f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36850bba3acc478a8dcdfe150506646b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28bcc301eb9d43e4816624adcbb014e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3331766463145cc9b5bb4aeb1c5f716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f3e17770d794c78968e38c3fbfa38b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "# tokenizer.eos_token = 198\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "def preprocess_function(x):\n",
    "    return tokenizer([e + \"\\n\" for e in x[\"text\"]])\n",
    "tokenized_eli5 = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=[\"text\"],\n",
    ")\n",
    "block_size = 128\n",
    "\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "lm_datasets = tokenized_eli5.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")\n",
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/gpu/langame-worker/env/lib/python3.8/site-packages/transformers/training_args.py:1186: FutureWarning: `--push_to_hub_model_id` and `--push_to_hub_organization` are deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_model_id` instead and pass the full repo name to this argument (in this case Langame/distilgpt2-starter-classification).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Tried to clone a repository in a non-empty folder that isn't a git repository. If you really want to do this, do it manually:\ngit init && git remote add origin && git pull origin main\n or clone repo to a new folder and move your existing files there afterwards.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu@91.175.184.114:49512/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb#ch0000019vscode-remote?line=0'>1</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu@91.175.184.114:49512/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb#ch0000019vscode-remote?line=1'>2</a>\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./results\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu@91.175.184.114:49512/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb#ch0000019vscode-remote?line=2'>3</a>\u001b[0m     evaluation_strategy\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu@91.175.184.114:49512/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb#ch0000019vscode-remote?line=9'>10</a>\u001b[0m     push_to_hub\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu@91.175.184.114:49512/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb#ch0000019vscode-remote?line=10'>11</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu@91.175.184.114:49512/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb#ch0000019vscode-remote?line=12'>13</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu@91.175.184.114:49512/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb#ch0000019vscode-remote?line=13'>14</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu@91.175.184.114:49512/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb#ch0000019vscode-remote?line=14'>15</a>\u001b[0m     args\u001b[39m=\u001b[39;49mtraining_args,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu@91.175.184.114:49512/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb#ch0000019vscode-remote?line=15'>16</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39;49mlm_datasets[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu@91.175.184.114:49512/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb#ch0000019vscode-remote?line=16'>17</a>\u001b[0m     eval_dataset\u001b[39m=\u001b[39;49mlm_datasets[\u001b[39m\"\u001b[39;49m\u001b[39meval\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu@91.175.184.114:49512/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb#ch0000019vscode-remote?line=17'>18</a>\u001b[0m     \u001b[39m# data_collator=data_collator,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu@91.175.184.114:49512/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb#ch0000019vscode-remote?line=18'>19</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu@91.175.184.114:49512/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb#ch0000019vscode-remote?line=20'>21</a>\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/langame-worker/env/lib/python3.8/site-packages/transformers/trainer.py:464\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39m# Create clone of distant repo and output directory if needed\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpush_to_hub:\n\u001b[0;32m--> 464\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minit_git_repo(at_init\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    465\u001b[0m     \u001b[39m# In case of pull, we need to make sure every process has the latest.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m     \u001b[39mif\u001b[39;00m is_torch_tpu_available():\n",
      "File \u001b[0;32m~/langame-worker/env/lib/python3.8/site-packages/transformers/trainer.py:3106\u001b[0m, in \u001b[0;36mTrainer.init_git_repo\u001b[0;34m(self, at_init)\u001b[0m\n\u001b[1;32m   3103\u001b[0m     repo_name \u001b[39m=\u001b[39m get_full_repo_name(repo_name, token\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mhub_token)\n\u001b[1;32m   3105\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3106\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepo \u001b[39m=\u001b[39m Repository(\n\u001b[1;32m   3107\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49moutput_dir,\n\u001b[1;32m   3108\u001b[0m         clone_from\u001b[39m=\u001b[39;49mrepo_name,\n\u001b[1;32m   3109\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   3110\u001b[0m         private\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mhub_private_repo,\n\u001b[1;32m   3111\u001b[0m     )\n\u001b[1;32m   3112\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[1;32m   3113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39moverwrite_output_dir \u001b[39mand\u001b[39;00m at_init:\n\u001b[1;32m   3114\u001b[0m         \u001b[39m# Try again after wiping output_dir\u001b[39;00m\n",
      "File \u001b[0;32m~/langame-worker/env/lib/python3.8/site-packages/huggingface_hub/repository.py:499\u001b[0m, in \u001b[0;36mRepository.__init__\u001b[0;34m(self, local_dir, clone_from, repo_type, use_auth_token, git_user, git_email, revision, private, skip_lfs_files, client)\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhuggingface_token \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[39mif\u001b[39;00m clone_from \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclone_from(repo_url\u001b[39m=\u001b[39;49mclone_from)\n\u001b[1;32m    500\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    501\u001b[0m     \u001b[39mif\u001b[39;00m is_git_repo(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocal_dir):\n",
      "File \u001b[0;32m~/langame-worker/env/lib/python3.8/site-packages/huggingface_hub/repository.py:727\u001b[0m, in \u001b[0;36mRepository.clone_from\u001b[0;34m(self, repo_url, use_auth_token)\u001b[0m\n\u001b[1;32m    724\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(error_msg)\n\u001b[1;32m    726\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m in_repository:\n\u001b[0;32m--> 727\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    728\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mTried to clone a repository in a non-empty folder that isn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt a\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    729\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m git repository. If you really want to do this, do it\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    730\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m manually:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mgit init && git remote add origin && git pull\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    731\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m origin main\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m or clone repo to a new folder and move your\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    732\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m existing files there afterwards.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    733\u001b[0m             )\n\u001b[1;32m    735\u001b[0m \u001b[39mexcept\u001b[39;00m subprocess\u001b[39m.\u001b[39mCalledProcessError \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    736\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(exc\u001b[39m.\u001b[39mstderr)\n",
      "\u001b[0;31mOSError\u001b[0m: Tried to clone a repository in a non-empty folder that isn't a git repository. If you really want to do this, do it manually:\ngit init && git remote add origin && git pull origin main\n or clone repo to a new folder and move your existing files there afterwards."
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    # learning_rate=2e-5,\n",
    "    # weight_decay=0.01,\n",
    "    num_train_epochs=2,\n",
    "    # auto_find_batch_size=True,\n",
    "    # push_to_hub_model_id=\"distilgpt2-starter-classification\",\n",
    "    # push_to_hub_organization=\"Langame\",\n",
    "    # push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"eval\"],\n",
    "    # data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 197.97\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:198 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'If an artificial intelligence were to request permission to exist, what would your decision be? ### artificial intelligence,ai,ai alignment\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "starter = \"If an artificial intelligence were to request permission to exist, what would your decision be? ###\"\n",
    "input_ids = torch.tensor([tokenizer.encode(starter)]).to(\"cuda\")\n",
    "beam_output = model.generate(\n",
    "    input_ids,  \n",
    "    max_length=50, \n",
    "    num_beams=5, \n",
    "    early_stopping=True,\n",
    "    eos_token_id=198,\n",
    ")\n",
    "tokenizer.decode(beam_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in local-pt-checkpoint/tokenizer_config.json\n",
      "Special tokens file saved in local-pt-checkpoint/special_tokens_map.json\n",
      "Configuration saved in local-pt-checkpoint/config.json\n",
      "Model weights saved in local-pt-checkpoint/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"local-pt-checkpoint\")\n",
    "model.save_pretrained(\"local-pt-checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using framework PyTorch: 1.12.0+cu116\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n",
      "/home/gpu/langame-worker/env/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:798: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if batch_size <= 0:\n",
      "Validating ONNX model...\n",
      "\t-[âœ“] ONNX model output names match reference model ({'logits'})\n",
      "\t- Validating ONNX Model output \"logits\":\n",
      "\t\t-[âœ“] (2, 8, 50257) matches (2, 8, 50257)\n",
      "\t\t-[âœ“] all values close (atol: 1e-05)\n",
      "All good, model saved at: onnx/model.onnx\n"
     ]
    }
   ],
   "source": [
    "!python -m transformers.onnx --model=local-pt-checkpoint onnx/ --feature=causal-lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv onnx/model.onnx distilgpt2-starter-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Tried to clone a repository in a non-empty folder that isn't a git repository. If you really want to do this, do it manually:\ngit init && git remote add origin && git pull origin main\n or clone repo to a new folder and move your existing files there afterwards.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu@91.175.184.114:49512/home/gpu/langame-worker/notebooks/fine_tune_classification.ipynb#ch0000023vscode-remote?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mpush_to_hub(\u001b[39m\"\u001b[39;49m\u001b[39mdistilgpt2-starter-classification\u001b[39;49m\u001b[39m\"\u001b[39;49m, private\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, organization\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mLangame\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/langame-worker/env/lib/python3.8/site-packages/transformers/trainer.py:3228\u001b[0m, in \u001b[0;36mTrainer.push_to_hub\u001b[0;34m(self, commit_message, blocking, **kwargs)\u001b[0m\n\u001b[1;32m   3225\u001b[0m \u001b[39m# If a user calls manually `push_to_hub` with `self.args.push_to_hub = False`, we try to create the repo but\u001b[39;00m\n\u001b[1;32m   3226\u001b[0m \u001b[39m# it might fail.\u001b[39;00m\n\u001b[1;32m   3227\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrepo\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 3228\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minit_git_repo()\n\u001b[1;32m   3230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mshould_save:\n\u001b[1;32m   3231\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mhub_model_id \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/langame-worker/env/lib/python3.8/site-packages/transformers/trainer.py:3106\u001b[0m, in \u001b[0;36mTrainer.init_git_repo\u001b[0;34m(self, at_init)\u001b[0m\n\u001b[1;32m   3103\u001b[0m     repo_name \u001b[39m=\u001b[39m get_full_repo_name(repo_name, token\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mhub_token)\n\u001b[1;32m   3105\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3106\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepo \u001b[39m=\u001b[39m Repository(\n\u001b[1;32m   3107\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49moutput_dir,\n\u001b[1;32m   3108\u001b[0m         clone_from\u001b[39m=\u001b[39;49mrepo_name,\n\u001b[1;32m   3109\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   3110\u001b[0m         private\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mhub_private_repo,\n\u001b[1;32m   3111\u001b[0m     )\n\u001b[1;32m   3112\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[1;32m   3113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39moverwrite_output_dir \u001b[39mand\u001b[39;00m at_init:\n\u001b[1;32m   3114\u001b[0m         \u001b[39m# Try again after wiping output_dir\u001b[39;00m\n",
      "File \u001b[0;32m~/langame-worker/env/lib/python3.8/site-packages/huggingface_hub/repository.py:499\u001b[0m, in \u001b[0;36mRepository.__init__\u001b[0;34m(self, local_dir, clone_from, repo_type, use_auth_token, git_user, git_email, revision, private, skip_lfs_files, client)\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhuggingface_token \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[39mif\u001b[39;00m clone_from \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclone_from(repo_url\u001b[39m=\u001b[39;49mclone_from)\n\u001b[1;32m    500\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    501\u001b[0m     \u001b[39mif\u001b[39;00m is_git_repo(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocal_dir):\n",
      "File \u001b[0;32m~/langame-worker/env/lib/python3.8/site-packages/huggingface_hub/repository.py:727\u001b[0m, in \u001b[0;36mRepository.clone_from\u001b[0;34m(self, repo_url, use_auth_token)\u001b[0m\n\u001b[1;32m    724\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(error_msg)\n\u001b[1;32m    726\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m in_repository:\n\u001b[0;32m--> 727\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    728\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mTried to clone a repository in a non-empty folder that isn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt a\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    729\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m git repository. If you really want to do this, do it\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    730\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m manually:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mgit init && git remote add origin && git pull\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    731\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m origin main\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m or clone repo to a new folder and move your\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    732\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m existing files there afterwards.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    733\u001b[0m             )\n\u001b[1;32m    735\u001b[0m \u001b[39mexcept\u001b[39;00m subprocess\u001b[39m.\u001b[39mCalledProcessError \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    736\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(exc\u001b[39m.\u001b[39mstderr)\n",
      "\u001b[0;31mOSError\u001b[0m: Tried to clone a repository in a non-empty folder that isn't a git repository. If you really want to do this, do it manually:\ngit init && git remote add origin && git pull origin main\n or clone repo to a new folder and move your existing files there afterwards."
     ]
    }
   ],
   "source": [
    "trainer.push_to_hub(\"distilgpt2-starter-classification\", private=True, organization=\"Langame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:198 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['artificial intelligence', 'ai alignment', 'ai']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Langame/distilgpt2-starter-classification\", use_auth_token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Langame/distilgpt2-starter-classification\", use_auth_token=True)\n",
    "tokenizer.eos_token_id = 198\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "device = \"cuda:0\" if True else \"cpu\"\n",
    "if device == \"cuda:0\":\n",
    "    model.cuda()\n",
    "import torch\n",
    "starter = \"If an artificial intelligence were to request permission to exist, what would your decision be? ###\"\n",
    "input_ids = torch.tensor([tokenizer.encode(starter)]).to(\"cuda\")\n",
    "beam_output = model.generate(\n",
    "    input_ids,  \n",
    "    return_dict_in_generate=True,\n",
    "    eos_token_id=198,  # line break\n",
    "    max_length=50,\n",
    "    num_return_sequences=1,\n",
    "    return_text=False,\n",
    "    return_full_text=False,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "topics = tokenizer.decode(beam_output[\"sequences\"].tolist()[0]).replace(\n",
    "    starter, \"\"\n",
    ").strip()  # TODO: return_text doesn't work for some reason\n",
    "list(set(topics.split(\",\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': ' history,ai,artificial intelligence\\n'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "API_KEY = \"YOUR_API_KEY\"\n",
    "API_URL = \"https://api-inference.huggingface.co/models/Langame/distilgpt2-starter-classification\"\n",
    "headers = {\"Authorization\": f\"Bearer {API_KEY}\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\n",
    "output = query({\n",
    "\t\"inputs\": starter,\n",
    "\t\"parameters\": {\n",
    "\t\t\"max_length\": 50,\n",
    "\t\t\"num_return_sequences\": 1,\n",
    "\t\t\"return_text\": False,\n",
    "\t\t\"return_full_text\": False,\n",
    "\t\t\"do_sample\": True,\n",
    "\t\t\"top_k\": 50,\n",
    "\t\t\"top_p\": 0.95,\n",
    "\t\t\"end_sequence\": \"\\n\",\n",
    "\t},\n",
    "\t\"options\": {\n",
    "\t\t\"wait_for_model\": True,\n",
    "\t\t\"use_cache\": True,  # TODO: should be in public api args\n",
    "\t},\n",
    "})\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ce36a526c2657a6901f63bfcfa96eb7dde3d51d13ec099443002e9cf8f1ac6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
